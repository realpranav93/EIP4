{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FINAL EIP4_assignment5_startercode.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/realpranav93/EIP4/blob/master/session5/FINAL_EIP4_assignment5_startercode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "5ad91a2f-aca4-4ae5-c2ff-e8732bc43e6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn726hpZo-vg",
        "colab_type": "code",
        "outputId": "bfaa185d-35b5-492f-a9a9-c05c8dc928f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!unzip -q \"/content/drive/My Drive/hvc_data.zip\"\n",
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title required packages\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation,GlobalAveragePooling2D,GlobalMaxPooling1D\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import *\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import os\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "3659b5af-95fd-4344-bf68-e32c9bf141e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head(),df.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              " 0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              " 1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              " 2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              " 3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              " 4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              " \n",
              " [5 rows x 9 columns], (13573, 9))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rKSAmGmpzcm",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Cutout implementation With Resnet Preprcess Implementation\n",
        "\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        from keras.applications.resnet50 import preprocess_input\n",
        "        input_img = preprocess_input(input_img)\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "        \n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-SUsjvudMJ9",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Augmentation With Cutout\n",
        "\n",
        "from keras.applications.resnet50 import preprocess_input\n",
        "datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=True,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=True,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=0.0,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        shear_range=0.1,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=1. / 255,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function= get_random_eraser(v_l=0, v_h=12),\n",
        "        #get_random_eraser(v_l=0, v_h=10)\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0\n",
        "    )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sHRtPkG2ocg",
        "colab_type": "code",
        "outputId": "1e967e6a-a542-4bf2-eaa8-a7569d1c19f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "_gender_cols_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gender_female', 'gender_male']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title datagenerator\n",
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator_new(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True,generator = datagen):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.generator = generator\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        if self.generator == None: \n",
        "          image_f = image \n",
        "        else:\n",
        "          self.generator.fit(image)\n",
        "          image_f = next(self.generator.flow(x = image,y = None,batch_size =self.batch_size))  \n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        return image_f, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "5c2e666b-6746-4905-c3af-ca057294ce1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator_new(train_df, batch_size=32,generator = datagen)\n",
        "valid_gen = PersonDataGenerator_new(val_df, batch_size=64,generator = datagen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "d1f49254-f9e8-458a-e12d-b0c2c048bc2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SELwVN-Y4UIT",
        "colab_type": "code",
        "outputId": "88844f53-d7b5-44dc-aab5-b417ae747bdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "images.shape,len(targets),len(targets['age_output']),targets['gender_output'][0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 224, 224, 3), 8, 32, array([0, 1], dtype=uint8))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUxueT_UHFHm",
        "colab_type": "code",
        "outputId": "f64b3e15-adc4-444d-80c3-410e1b4ad089",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "images[11].max()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.61409986"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de9fsC1SodiR",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title accuracy function\n",
        "def accuracy(test_x, test_y, model):\n",
        "    result = model.predict(test_x)\n",
        "    predicted_class = np.argmax(result, axis=1)\n",
        "    true_class = np.argmax(test_y, axis=1)\n",
        "    num_correct = np.sum(predicted_class == true_class) \n",
        "    accuracy = float(num_correct)/result.shape[0]\n",
        "    return (accuracy * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyVUcDh-pvKq",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title LR implementation - reference from Github\n",
        "from keras.callbacks import Callback\n",
        "#Have taken this class from https://github.com/titu1994/keras-one-cycle\n",
        "class OneCycleLR(Callback):\n",
        "    def __init__(self,\n",
        "                 epochs,\n",
        "                 batch_size,\n",
        "                 samples,\n",
        "                 steps,\n",
        "                 max_lr,\n",
        "                 end_percentage=0.1,\n",
        "                 scale_percentage=None,\n",
        "                 maximum_momentum=0.95,\n",
        "                 minimum_momentum=0.85,\n",
        "                 verbose=True):\n",
        "        \"\"\" This callback implements a cyclical learning rate policy (CLR).\n",
        "        This is a special case of Cyclic Learning Rates, where we have only 1 cycle.\n",
        "        After the completion of 1 cycle, the learning rate will decrease rapidly to\n",
        "        100th its initial lowest value.\n",
        "        # Arguments:\n",
        "            max_lr: Float. Initial learning rate. This also sets the\n",
        "                starting learning rate (which will be 10x smaller than\n",
        "                this), and will increase to this value during the first cycle.\n",
        "            end_percentage: Float. The percentage of all the epochs of training\n",
        "                that will be dedicated to sharply decreasing the learning\n",
        "                rate after the completion of 1 cycle. Must be between 0 and 1.\n",
        "            scale_percentage: Float or None. If float, must be between 0 and 1.\n",
        "                If None, it will compute the scale_percentage automatically\n",
        "                based on the `end_percentage`.\n",
        "            maximum_momentum: Optional. Sets the maximum momentum (initial)\n",
        "                value, which gradually drops to its lowest value in half-cycle,\n",
        "                then gradually increases again to stay constant at this max value.\n",
        "                Can only be used with SGD Optimizer.\n",
        "            minimum_momentum: Optional. Sets the minimum momentum at the end of\n",
        "                the half-cycle. Can only be used with SGD Optimizer.\n",
        "            verbose: Bool. Whether to print the current learning rate after every\n",
        "                epoch.\n",
        "        # Reference\n",
        "            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n",
        "            - [Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120)\n",
        "        \"\"\"\n",
        "        super(OneCycleLR, self).__init__()\n",
        "\n",
        "        if end_percentage < 0. or end_percentage > 1.:\n",
        "            raise ValueError(\"`end_percentage` must be between 0 and 1\")\n",
        "\n",
        "        if scale_percentage is not None and (scale_percentage < 0. or scale_percentage > 1.):\n",
        "            raise ValueError(\"`scale_percentage` must be between 0 and 1\")\n",
        "\n",
        "        self.initial_lr = max_lr\n",
        "        self.end_percentage = end_percentage\n",
        "        self.scale = float(scale_percentage) if scale_percentage is not None else float(end_percentage)\n",
        "        self.max_momentum = maximum_momentum\n",
        "        self.min_momentum = minimum_momentum\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if self.max_momentum is not None and self.min_momentum is not None:\n",
        "            self._update_momentum = True\n",
        "        else:\n",
        "            self._update_momentum = False\n",
        "\n",
        "        self.clr_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.samples = samples\n",
        "        self.steps = steps\n",
        "        self.num_iterations = None\n",
        "        self.mid_cycle_id = None\n",
        "\n",
        "    def _reset(self):\n",
        "        \"\"\"\n",
        "        Reset the callback.\n",
        "        \"\"\"\n",
        "        self.clr_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "    def compute_lr(self):\n",
        "        \"\"\"\n",
        "        Compute the learning rate based on which phase of the cycle it is in.\n",
        "        - If in the first half of training, the learning rate gradually increases.\n",
        "        - If in the second half of training, the learning rate gradually decreases.\n",
        "        - If in the final `end_percentage` portion of training, the learning rate\n",
        "            is quickly reduced to near 100th of the original min learning rate.\n",
        "        # Returns:\n",
        "            the new learning rate\n",
        "        \"\"\"\n",
        "        if self.clr_iterations > 2 * self.mid_cycle_id:\n",
        "            current_percentage = (self.clr_iterations - 2 * self.mid_cycle_id)\n",
        "            current_percentage /= float((self.num_iterations - 2 * self.mid_cycle_id))\n",
        "            new_lr = self.initial_lr * (1. + (current_percentage *\n",
        "                                              (1. - 100.) / 100.)) * self.scale\n",
        "\n",
        "        elif self.clr_iterations > self.mid_cycle_id:\n",
        "            current_percentage = 1. - (\n",
        "                self.clr_iterations - self.mid_cycle_id) / self.mid_cycle_id\n",
        "            new_lr = self.initial_lr * (1. + current_percentage *\n",
        "                                        (self.scale * 100 - 1.)) * self.scale\n",
        "\n",
        "        else:\n",
        "            current_percentage = self.clr_iterations / self.mid_cycle_id\n",
        "            new_lr = self.initial_lr * (1. + current_percentage *\n",
        "                                        (self.scale * 100 - 1.)) * self.scale\n",
        "\n",
        "        if self.clr_iterations == self.num_iterations:\n",
        "            self.clr_iterations = 0\n",
        "\n",
        "        return new_lr\n",
        "\n",
        "    def compute_momentum(self):\n",
        "        \"\"\"\n",
        "         Compute the momentum based on which phase of the cycle it is in.\n",
        "        - If in the first half of training, the momentum gradually decreases.\n",
        "        - If in the second half of training, the momentum gradually increases.\n",
        "        - If in the final `end_percentage` portion of training, the momentum value\n",
        "            is kept constant at the maximum initial value.\n",
        "        # Returns:\n",
        "            the new momentum value\n",
        "        \"\"\"\n",
        "        if self.clr_iterations > 2 * self.mid_cycle_id:\n",
        "            new_momentum = self.max_momentum\n",
        "\n",
        "        elif self.clr_iterations > self.mid_cycle_id:\n",
        "            current_percentage = 1. - ((self.clr_iterations - self.mid_cycle_id) / float(\n",
        "                                        self.mid_cycle_id))\n",
        "            new_momentum = self.max_momentum - current_percentage * (\n",
        "                self.max_momentum - self.min_momentum)\n",
        "\n",
        "        else:\n",
        "            current_percentage = self.clr_iterations / float(self.mid_cycle_id)\n",
        "            new_momentum = self.max_momentum - current_percentage * (\n",
        "                self.max_momentum - self.min_momentum)\n",
        "\n",
        "        return new_momentum\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        \n",
        "        if self.steps is not None:\n",
        "            self.num_iterations = self.epochs * self.steps\n",
        "        else:\n",
        "            if (self.samples % self.batch_size) == 0:\n",
        "                remainder = 0\n",
        "            else:\n",
        "                remainder = 1\n",
        "            self.num_iterations = (self.epochs + remainder) * self.samples // self.batch_size\n",
        "\n",
        "        self.mid_cycle_id = int(self.num_iterations * ((1. - self.end_percentage)) / float(2))\n",
        "\n",
        "        self._reset()\n",
        "        K.set_value(self.model.optimizer.lr, self.compute_lr())\n",
        "\n",
        "        if self._update_momentum:\n",
        "            if not hasattr(self.model.optimizer, 'momentum'):\n",
        "                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n",
        "\n",
        "            new_momentum = self.compute_momentum()\n",
        "            K.set_value(self.model.optimizer.momentum, new_momentum)\n",
        "\n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "\n",
        "        self.clr_iterations += 1\n",
        "        new_lr = self.compute_lr()\n",
        "\n",
        "        self.history.setdefault('lr', []).append(\n",
        "            K.get_value(self.model.optimizer.lr))\n",
        "        K.set_value(self.model.optimizer.lr, new_lr)\n",
        "\n",
        "        if self._update_momentum:\n",
        "            if not hasattr(self.model.optimizer, 'momentum'):\n",
        "                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n",
        "\n",
        "            new_momentum = self.compute_momentum()\n",
        "\n",
        "            self.history.setdefault('momentum', []).append(\n",
        "                K.get_value(self.model.optimizer.momentum))\n",
        "            K.set_value(self.model.optimizer.momentum, new_momentum)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if self.verbose:\n",
        "            if self._update_momentum:\n",
        "                print(\" - lr: %0.5f - momentum: %0.2f \" %\n",
        "                      (self.history['lr'][-1], self.history['momentum'][-1]))\n",
        "\n",
        "            else:\n",
        "                print(\" - lr: %0.5f \" % (self.history['lr'][-1]))\n",
        "\n",
        "\n",
        "class LRFinder(Callback):\n",
        "    def __init__(self,\n",
        "                 num_samples,\n",
        "                 batch_size,\n",
        "                 minimum_lr=1e-5,\n",
        "                 maximum_lr=10.,\n",
        "                 lr_scale='exp',\n",
        "                 validation_data=None,\n",
        "                 validation_sample_rate=5,\n",
        "                 stopping_criterion_factor=4.,\n",
        "                 loss_smoothing_beta=0.98,\n",
        "                 save_dir=None,\n",
        "                 verbose=True):\n",
        "        \"\"\"\n",
        "        This class uses the Cyclic Learning Rate history to find a\n",
        "        set of learning rates that can be good initializations for the\n",
        "        One-Cycle training proposed by Leslie Smith in the paper referenced\n",
        "        below.\n",
        "        A port of the Fast.ai implementation for Keras.\n",
        "        # Note\n",
        "        This requires that the model be trained for exactly 1 epoch. If the model\n",
        "        is trained for more epochs, then the metric calculations are only done for\n",
        "        the first epoch.\n",
        "        # Interpretation\n",
        "        Upon visualizing the loss plot, check where the loss starts to increase\n",
        "        rapidly. Choose a learning rate at somewhat prior to the corresponding\n",
        "        position in the plot for faster convergence. This will be the maximum_lr lr.\n",
        "        Choose the max value as this value when passing the `max_val` argument\n",
        "        to OneCycleLR callback.\n",
        "        Since the plot is in log-scale, you need to compute 10 ^ (-k) of the x-axis\n",
        "        # Arguments:\n",
        "            num_samples: Integer. Number of samples in the dataset.\n",
        "            batch_size: Integer. Batch size during training.\n",
        "            minimum_lr: Float. Initial learning rate (and the minimum).\n",
        "            maximum_lr: Float. Final learning rate (and the maximum).\n",
        "            lr_scale: Can be one of ['exp', 'linear']. Chooses the type of\n",
        "                scaling for each update to the learning rate during subsequent\n",
        "                batches. Choose 'exp' for large range and 'linear' for small range.\n",
        "            validation_data: Requires the validation dataset as a tuple of\n",
        "                (X, y) belonging to the validation set. If provided, will use the\n",
        "                validation set to compute the loss metrics. Else uses the training\n",
        "                batch loss. Will warn if not provided to alert the user.\n",
        "            validation_sample_rate: Positive or Negative Integer. Number of batches to sample from the\n",
        "                validation set per iteration of the LRFinder. Larger number of\n",
        "                samples will reduce the variance but will take longer time to execute\n",
        "                per batch.\n",
        "                If Positive > 0, will sample from the validation dataset\n",
        "                If Megative, will use the entire dataset\n",
        "            stopping_criterion_factor: Integer or None. A factor which is used\n",
        "                to measure large increase in the loss value during training.\n",
        "                Since callbacks cannot stop training of a model, it will simply\n",
        "                stop logging the additional values from the epochs after this\n",
        "                stopping criterion has been met.\n",
        "                If None, this check will not be performed.\n",
        "            loss_smoothing_beta: Float. The smoothing factor for the moving\n",
        "                average of the loss function.\n",
        "            save_dir: Optional, String. If passed a directory path, the callback\n",
        "                will save the running loss and learning rates to two separate numpy\n",
        "                arrays inside this directory. If the directory in this path does not\n",
        "                exist, they will be created.\n",
        "            verbose: Whether to print the learning rate after every batch of training.\n",
        "        # References:\n",
        "            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n",
        "        \"\"\"\n",
        "        super(LRFinder, self).__init__()\n",
        "\n",
        "        if lr_scale not in ['exp', 'linear']:\n",
        "            raise ValueError(\"`lr_scale` must be one of ['exp', 'linear']\")\n",
        "\n",
        "        if validation_data is not None:\n",
        "            self.validation_data = validation_data\n",
        "            self.use_validation_set = True\n",
        "\n",
        "            if validation_sample_rate > 0 or validation_sample_rate < 0:\n",
        "                self.validation_sample_rate = validation_sample_rate\n",
        "            else:\n",
        "                raise ValueError(\"`validation_sample_rate` must be a positive or negative integer other than o\")\n",
        "        else:\n",
        "            self.use_validation_set = False\n",
        "            self.validation_sample_rate = 0\n",
        "\n",
        "        self.num_samples = num_samples\n",
        "        self.batch_size = batch_size\n",
        "        self.initial_lr = minimum_lr\n",
        "        self.final_lr = maximum_lr\n",
        "        self.lr_scale = lr_scale\n",
        "        self.stopping_criterion_factor = stopping_criterion_factor\n",
        "        self.loss_smoothing_beta = loss_smoothing_beta\n",
        "        self.save_dir = save_dir\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.num_batches_ = num_samples // batch_size\n",
        "        self.current_lr_ = minimum_lr\n",
        "\n",
        "        if lr_scale == 'exp':\n",
        "            self.lr_multiplier_ = (maximum_lr / float(minimum_lr)) ** (\n",
        "                1. / float(self.num_batches_))\n",
        "        else:\n",
        "            extra_batch = int((num_samples % batch_size) != 0)\n",
        "            self.lr_multiplier_ = np.linspace(\n",
        "                minimum_lr, maximum_lr, num=self.num_batches_ + extra_batch)\n",
        "\n",
        "        # If negative, use entire validation set\n",
        "        if self.validation_sample_rate < 0:\n",
        "            self.validation_sample_rate = self.validation_data[0].shape[0] // batch_size\n",
        "\n",
        "        self.current_batch_ = 0\n",
        "        self.current_epoch_ = 0\n",
        "        self.best_loss_ = 1e6\n",
        "        self.running_loss_ = 0.\n",
        "\n",
        "        self.history = {}\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "\n",
        "        self.current_epoch_ = 1\n",
        "        K.set_value(self.model.optimizer.lr, self.initial_lr)\n",
        "\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.current_batch_ = 0\n",
        "\n",
        "        if self.current_epoch_ > 1:\n",
        "            warnings.warn(\n",
        "                \"\\n\\nLearning rate finder should be used only with a single epoch. \"\n",
        "                \"Hereafter, the callback will not measure the losses.\\n\\n\")\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        self.current_batch_ += 1\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        if self.current_epoch_ > 1:\n",
        "            return\n",
        "\n",
        "        if self.use_validation_set:\n",
        "            X, Y = self.validation_data[0], self.validation_data[1]\n",
        "\n",
        "            # use 5 random batches from test set for fast approximate of loss\n",
        "            num_samples = self.batch_size * self.validation_sample_rate\n",
        "\n",
        "            if num_samples > X.shape[0]:\n",
        "                num_samples = X.shape[0]\n",
        "\n",
        "            idx = np.random.choice(X.shape[0], num_samples, replace=False)\n",
        "            x = X[idx]\n",
        "            y = Y[idx]\n",
        "\n",
        "            values = self.model.evaluate(x, y, batch_size=self.batch_size, verbose=False)\n",
        "            loss = values[0]\n",
        "        else:\n",
        "            loss = logs['loss']\n",
        "\n",
        "        # smooth the loss value and bias correct\n",
        "        running_loss = self.loss_smoothing_beta * loss + (\n",
        "            1. - self.loss_smoothing_beta) * loss\n",
        "        running_loss = running_loss / (\n",
        "            1. - self.loss_smoothing_beta**self.current_batch_)\n",
        "\n",
        "        # stop logging if loss is too large\n",
        "        if self.current_batch_ > 1 and self.stopping_criterion_factor is not None and (\n",
        "                running_loss >\n",
        "                self.stopping_criterion_factor * self.best_loss_):\n",
        "\n",
        "            if self.verbose:\n",
        "                print(\" - LRFinder: Skipping iteration since loss is %d times as large as best loss (%0.4f)\"\n",
        "                      % (self.stopping_criterion_factor, self.best_loss_))\n",
        "            return\n",
        "\n",
        "        if running_loss < self.best_loss_ or self.current_batch_ == 1:\n",
        "            self.best_loss_ = running_loss\n",
        "\n",
        "        current_lr = K.get_value(self.model.optimizer.lr)\n",
        "\n",
        "        self.history.setdefault('running_loss_', []).append(running_loss)\n",
        "        if self.lr_scale == 'exp':\n",
        "            self.history.setdefault('log_lrs', []).append(np.log10(current_lr))\n",
        "        else:\n",
        "            self.history.setdefault('log_lrs', []).append(current_lr)\n",
        "\n",
        "        # compute the lr for the next batch and update the optimizer lr\n",
        "        if self.lr_scale == 'exp':\n",
        "            current_lr *= self.lr_multiplier_\n",
        "        else:\n",
        "            current_lr = self.lr_multiplier_[self.current_batch_ - 1]\n",
        "\n",
        "        K.set_value(self.model.optimizer.lr, current_lr)\n",
        "\n",
        "        # save the other metrics as well\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "        if self.verbose:\n",
        "            if self.use_validation_set:\n",
        "                print(\" - LRFinder: val_loss: %1.4f - lr = %1.8f \" %\n",
        "                      (values[0], current_lr))\n",
        "            else:\n",
        "                print(\" - LRFinder: lr = %1.8f \" % current_lr)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if self.save_dir is not None and self.current_epoch_ <= 1:\n",
        "            if not os.path.exists(self.save_dir):\n",
        "                os.makedirs(self.save_dir)\n",
        "\n",
        "            losses_path = os.path.join(self.save_dir, 'losses.npy')\n",
        "            lrs_path = os.path.join(self.save_dir, 'lrs.npy')\n",
        "\n",
        "            np.save(losses_path, self.losses)\n",
        "            np.save(lrs_path, self.lrs)\n",
        "\n",
        "            if self.verbose:\n",
        "                print(\"\\tLR Finder : Saved the losses and learning rate values in path : {%s}\"\n",
        "                      % (self.save_dir))\n",
        "\n",
        "        self.current_epoch_ += 1\n",
        "\n",
        "        warnings.simplefilter(\"default\")\n",
        "\n",
        "    def plot_schedule(self, clip_beginning=None, clip_endding=None):\n",
        "        \"\"\"\n",
        "        Plots the schedule from the callback itself.\n",
        "        # Arguments:\n",
        "            clip_beginning: Integer or None. If positive integer, it will\n",
        "                remove the specified portion of the loss graph to remove the large\n",
        "                loss values in the beginning of the graph.\n",
        "            clip_endding: Integer or None. If negative integer, it will\n",
        "                remove the specified portion of the ending of the loss graph to\n",
        "                remove the sharp increase in the loss values at high learning rates.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            import matplotlib.pyplot as plt\n",
        "            plt.style.use('seaborn-white')\n",
        "        except ImportError:\n",
        "            print(\n",
        "                \"Matplotlib not found. Please use `pip install matplotlib` first.\"\n",
        "            )\n",
        "            return\n",
        "\n",
        "        if clip_beginning is not None and clip_beginning < 0:\n",
        "            clip_beginning = -clip_beginning\n",
        "\n",
        "        if clip_endding is not None and clip_endding > 0:\n",
        "            clip_endding = -clip_endding\n",
        "\n",
        "        losses = self.losses\n",
        "        lrs = self.lrs\n",
        "\n",
        "        if clip_beginning:\n",
        "            losses = losses[clip_beginning:]\n",
        "            lrs = lrs[clip_beginning:]\n",
        "\n",
        "        if clip_endding:\n",
        "            losses = losses[:clip_endding]\n",
        "            lrs = lrs[:clip_endding]\n",
        "\n",
        "        plt.plot(lrs, losses)\n",
        "        plt.title('Learning rate vs Loss')\n",
        "        plt.xlabel('learning rate')\n",
        "        plt.ylabel('loss')\n",
        "        plt.show()\n",
        "\n",
        "    @classmethod\n",
        "    def restore_schedule_from_dir(cls,\n",
        "                                  directory,\n",
        "                                  clip_beginning=None,\n",
        "                                  clip_endding=None):\n",
        "        \"\"\"\n",
        "        Loads the training history from the saved numpy files in the given directory.\n",
        "        # Arguments:\n",
        "            directory: String. Path to the directory where the serialized numpy\n",
        "                arrays of the loss and learning rates are saved.\n",
        "            clip_beginning: Integer or None. If positive integer, it will\n",
        "                remove the specified portion of the loss graph to remove the large\n",
        "                loss values in the beginning of the graph.\n",
        "            clip_endding: Integer or None. If negative integer, it will\n",
        "                remove the specified portion of the ending of the loss graph to\n",
        "                remove the sharp increase in the loss values at high learning rates.\n",
        "        Returns:\n",
        "            tuple of (losses, learning rates)\n",
        "        \"\"\"\n",
        "        if clip_beginning is not None and clip_beginning < 0:\n",
        "            clip_beginning = -clip_beginning\n",
        "\n",
        "        if clip_endding is not None and clip_endding > 0:\n",
        "            clip_endding = -clip_endding\n",
        "\n",
        "        losses_path = os.path.join(directory, 'losses.npy')\n",
        "        lrs_path = os.path.join(directory, 'lrs.npy')\n",
        "\n",
        "        if not os.path.exists(losses_path) or not os.path.exists(lrs_path):\n",
        "            print(\"%s and %s could not be found at directory : {%s}\" %\n",
        "                  (losses_path, lrs_path, directory))\n",
        "\n",
        "            losses = None\n",
        "            lrs = None\n",
        "\n",
        "        else:\n",
        "            losses = np.load(losses_path)\n",
        "            lrs = np.load(lrs_path)\n",
        "\n",
        "            if clip_beginning:\n",
        "                losses = losses[clip_beginning:]\n",
        "                lrs = lrs[clip_beginning:]\n",
        "\n",
        "            if clip_endding:\n",
        "                losses = losses[:clip_endding]\n",
        "                lrs = lrs[:clip_endding]\n",
        "\n",
        "        return losses, lrs\n",
        "\n",
        "    @classmethod\n",
        "    def plot_schedule_from_file(cls,\n",
        "                                directory,\n",
        "                                clip_beginning=None,\n",
        "                                clip_endding=None):\n",
        "        \"\"\"\n",
        "        Plots the schedule from the saved numpy arrays of the loss and learning\n",
        "        rate values in the specified directory.\n",
        "        # Arguments:\n",
        "            directory: String. Path to the directory where the serialized numpy\n",
        "                arrays of the loss and learning rates are saved.\n",
        "            clip_beginning: Integer or None. If positive integer, it will\n",
        "                remove the specified portion of the loss graph to remove the large\n",
        "                loss values in the beginning of the graph.\n",
        "            clip_endding: Integer or None. If negative integer, it will\n",
        "                remove the specified portion of the ending of the loss graph to\n",
        "                remove the sharp increase in the loss values at high learning rates.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            import matplotlib.pyplot as plt\n",
        "            plt.style.use('seaborn-white')\n",
        "        except ImportError:\n",
        "            print(\"Matplotlib not found. Please use `pip install matplotlib` first.\")\n",
        "            return\n",
        "\n",
        "        losses, lrs = cls.restore_schedule_from_dir(\n",
        "            directory,\n",
        "            clip_beginning=clip_beginning,\n",
        "            clip_endding=clip_endding)\n",
        "\n",
        "        if losses is None or lrs is None:\n",
        "            return\n",
        "        else:\n",
        "            plt.plot(lrs, losses)\n",
        "            plt.title('Learning rate vs Loss')\n",
        "            plt.xlabel('learning rate')\n",
        "            plt.ylabel('loss')\n",
        "            plt.show()\n",
        "\n",
        "    @property\n",
        "    def lrs(self):\n",
        "        return np.array(self.history['log_lrs'])\n",
        "\n",
        "    @property\n",
        "    def losses(self):\n",
        "        return np.array(self.history['running_loss_'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmJDY39JSWNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.resnet50 import preprocess_input, decode_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdJ6seCGHf0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img = Input(shape = (224, 224, 3))\n",
        "model = ResNet50(\n",
        "weights = None,\n",
        "include_top = False, \n",
        "input_tensor = img, \n",
        "input_shape = None, \n",
        "pooling = 'avg'\n",
        ")\n",
        "neck = model.layers[-1].output\n",
        "#dense_layer_1 = Dense(128, activation = 'relu')(neck)\n",
        "def build_head_softmax(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\",kernel_initializer='he_normal'\n",
        "    )(in_layer)\n",
        "\n",
        "def build_head_sigmoid(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"sigmoid\", name=f\"{name}_output\",kernel_initializer='he_normal'\n",
        "    )(in_layer)\n",
        "\n",
        "\n",
        "# heads\n",
        "gender = build_head_softmax(\"gender\", neck)\n",
        "image_quality = build_head_softmax(\"image_quality\", neck)\n",
        "age = build_head_softmax(\"age\",neck)\n",
        "weight = build_head_softmax(\"weight\",neck)\n",
        "bag = build_head_softmax(\"bag\",neck)\n",
        "footwear = build_head_softmax(\"footwear\", neck)\n",
        "emotion = build_head_softmax(\"emotion\",neck)\n",
        "pose = build_head_softmax(\"pose\", neck)\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=img, \n",
        "    outputs = [gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RLSr6OXWFz1",
        "colab_type": "code",
        "outputId": "057cb6d5-da93-4ff6-f179-6e293cf14cd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer= Adam(),#SGD(lr=0.001, momentum=0.9),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv1 (BatchNormalization)   (None, 112, 112, 64) 256         conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 112, 112, 64) 0           bn_conv1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2a (Conv2D)         (None, 56, 56, 64)   4160        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 56, 56, 64)   0           bn2a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 56, 56, 64)   0           bn2a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch1 (Conv2D)          (None, 56, 56, 256)  16640       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch1 (BatchNormalizatio (None, 56, 56, 256)  1024        res2a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 56, 56, 256)  0           bn2a_branch2c[0][0]              \n",
            "                                                                 bn2a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 56, 56, 256)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2a (Conv2D)         (None, 56, 56, 64)   16448       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 56, 56, 64)   0           bn2b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 56, 56, 64)   0           bn2b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 56, 56, 256)  0           bn2b_branch2c[0][0]              \n",
            "                                                                 activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 56, 56, 256)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2a (Conv2D)         (None, 56, 56, 64)   16448       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 56, 56, 64)   0           bn2c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 56, 56, 64)   0           bn2c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 56, 56, 256)  0           bn2c_branch2c[0][0]              \n",
            "                                                                 activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 56, 56, 256)  0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2a (Conv2D)         (None, 28, 28, 128)  32896       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 28, 28, 128)  0           bn3a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 28, 28, 128)  0           bn3a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch1 (Conv2D)          (None, 28, 28, 512)  131584      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch1 (BatchNormalizatio (None, 28, 28, 512)  2048        res3a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 28, 28, 512)  0           bn3a_branch2c[0][0]              \n",
            "                                                                 bn3a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 28, 28, 512)  0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 28, 28, 128)  0           bn3b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 28, 28, 128)  0           bn3b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 28, 28, 512)  0           bn3b_branch2c[0][0]              \n",
            "                                                                 activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 28, 28, 512)  0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 28, 28, 128)  0           bn3c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 28, 28, 128)  0           bn3c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 28, 28, 512)  0           bn3c_branch2c[0][0]              \n",
            "                                                                 activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 28, 28, 512)  0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3d_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 28, 28, 128)  0           bn3d_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3d_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 28, 28, 128)  0           bn3d_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3d_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 28, 28, 512)  0           bn3d_branch2c[0][0]              \n",
            "                                                                 activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 28, 28, 512)  0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2a (Conv2D)         (None, 14, 14, 256)  131328      activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 14, 14, 256)  0           bn4a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 14, 14, 256)  0           bn4a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch1 (Conv2D)          (None, 14, 14, 1024) 525312      activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch1 (BatchNormalizatio (None, 14, 14, 1024) 4096        res4a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 14, 14, 1024) 0           bn4a_branch2c[0][0]              \n",
            "                                                                 bn4a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 14, 14, 1024) 0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 14, 14, 256)  0           bn4b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 14, 14, 256)  0           bn4b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 14, 14, 1024) 0           bn4b_branch2c[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 14, 14, 1024) 0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 14, 14, 256)  0           bn4c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 14, 14, 256)  0           bn4c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 14, 14, 1024) 0           bn4c_branch2c[0][0]              \n",
            "                                                                 activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 14, 14, 1024) 0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4d_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 14, 14, 256)  0           bn4d_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4d_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 14, 14, 256)  0           bn4d_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4d_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 14, 14, 1024) 0           bn4d_branch2c[0][0]              \n",
            "                                                                 activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 14, 14, 1024) 0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4e_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 14, 14, 256)  0           bn4e_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4e_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 14, 14, 256)  0           bn4e_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4e_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 14, 14, 1024) 0           bn4e_branch2c[0][0]              \n",
            "                                                                 activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 14, 14, 1024) 0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4f_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 14, 14, 256)  0           bn4f_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4f_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 14, 14, 256)  0           bn4f_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4f_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 14, 14, 1024) 0           bn4f_branch2c[0][0]              \n",
            "                                                                 activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 14, 14, 1024) 0           add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2a (Conv2D)         (None, 7, 7, 512)    524800      activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 7, 7, 512)    0           bn5a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 7, 7, 512)    0           bn5a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch1 (Conv2D)          (None, 7, 7, 2048)   2099200     activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch1 (BatchNormalizatio (None, 7, 7, 2048)   8192        res5a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 7, 7, 2048)   0           bn5a_branch2c[0][0]              \n",
            "                                                                 bn5a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 7, 7, 2048)   0           add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 7, 7, 512)    0           bn5b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 7, 7, 512)    0           bn5b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 7, 7, 2048)   0           bn5b_branch2c[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 7, 7, 2048)   0           add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 7, 7, 512)    0           bn5c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 7, 7, 512)    0           bn5c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 7, 7, 2048)   0           bn5c_branch2c[0][0]              \n",
            "                                                                 activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 7, 7, 2048)   0           add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 2048)         0           activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "gender_output (Dense)           (None, 2)            4098        global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "image_quality_output (Dense)    (None, 3)            6147        global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "age_output (Dense)              (None, 5)            10245       global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "weight_output (Dense)           (None, 4)            8196        global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "bag_output (Dense)              (None, 3)            6147        global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "footwear_output (Dense)         (None, 3)            6147        global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "pose_output (Dense)             (None, 3)            6147        global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "emotion_output (Dense)          (None, 4)            8196        global_average_pooling2d_1[0][0] \n",
            "==================================================================================================\n",
            "Total params: 23,643,035\n",
            "Trainable params: 23,589,915\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXXeMmFYYH_6",
        "colab_type": "code",
        "outputId": "2ff4e8b5-59ad-4438-9736-04fdb7b55600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11537, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PWveDFBqKlM",
        "colab_type": "code",
        "outputId": "d0967d8f-056d-463e-a869-e85c03afc678",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "model2 = Model(\n",
        "    inputs=img, \n",
        "    outputs = [gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")\n",
        "model2.compile(loss='categorical_crossentropy',\n",
        "              optimizer= Adam(),#SGD(lr=0.001, momentum=0.9),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "lr_callback = LRFinder(num_samples=train_df.shape[0], batch_size=64,\n",
        "                       minimum_lr=0.00002, maximum_lr=100,verbose=False,\n",
        "                       lr_scale='exp', save_dir='./LR_LOGS/')\n",
        "\n",
        "\n",
        "# Need to have the epochs = 1 here.\n",
        "train_history = model2.fit_generator(train_gen, \n",
        "                                    #use_multiprocessing=True,\n",
        "                                    workers=6, \n",
        "                                    epochs=1, \n",
        "                                    validation_data = valid_gen, \n",
        "                                    callbacks=[lr_callback],\n",
        "                                    verbose=1)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "360/360 [==============================] - 223s 619ms/step - loss: 46.9579 - gender_output_loss: 4.9048 - image_quality_output_loss: 7.6280 - age_output_loss: 8.8379 - weight_output_loss: 4.0950 - bag_output_loss: 7.0761 - footwear_output_loss: 6.8945 - pose_output_loss: 4.2086 - emotion_output_loss: 3.3130 - gender_output_acc: 0.5363 - image_quality_output_acc: 0.3709 - age_output_acc: 0.2510 - weight_output_acc: 0.6261 - bag_output_acc: 0.4038 - footwear_output_acc: 0.3822 - pose_output_acc: 0.6107 - emotion_output_acc: 0.7062 - val_loss: 71.3534 - val_gender_output_loss: 6.8486 - val_image_quality_output_loss: 11.5443 - val_age_output_loss: 11.8773 - val_weight_output_loss: 5.9712 - val_bag_output_loss: 10.7562 - val_footwear_output_loss: 13.1691 - val_pose_output_loss: 6.4424 - val_emotion_output_loss: 4.7444 - val_gender_output_acc: 0.5751 - val_image_quality_output_acc: 0.2838 - val_age_output_acc: 0.2631 - val_weight_output_acc: 0.6295 - val_bag_output_acc: 0.3327 - val_footwear_output_acc: 0.1830 - val_pose_output_acc: 0.6003 - val_emotion_output_acc: 0.7056\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU3yWzFzZfPU",
        "colab_type": "code",
        "outputId": "3abaa708-7f41-4563-a7b1-1dc4eee5460a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        }
      },
      "source": [
        "lr_callback.plot_schedule(clip_beginning=1)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAESCAYAAAAVLtXjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW4k16yeZjDC",
        "colab_type": "code",
        "outputId": "bebeab0e-a679-4d06-ce8b-c5b05a225663",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        }
      },
      "source": [
        "lr_callback.plot_schedule(clip_beginning=30)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAESCAYAAAAYMKWkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krPgkHu7m5A2",
        "colab_type": "code",
        "outputId": "6fece34b-0db3-40eb-f179-977970631b2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "max_lr = 10**-2\n",
        "max_lr"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_RcY0Xe6rpN",
        "colab_type": "text"
      },
      "source": [
        "#FINAL TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha0oEDo8_Q3i",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title lr_schedule\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 100:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 60 :\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 40 :\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 25:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_dir = os.path.join('/content/drive/My Drive/EIP/', 'saved_models')\n",
        "model_name = 'assign5n_%s_model.{epoch:03d}.h5'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "opt = SGD(lr=0.001, momentum=0.9)#Adam() #\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpxv41EyNmN4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7c7232c9-cc3f-4d0c-857a-4dba8d52834b"
      },
      "source": [
        "epochs = 50\n",
        "batch_size = 32\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True,\n",
        "                             mode = 'min')\n",
        "lr_manager = OneCycleLR(samples=train_df.shape[0], epochs=epochs, batch_size=32,\n",
        "                    steps=len(train_gen), max_lr=max_lr,\n",
        "                    end_percentage=0.1, scale_percentage=None,\n",
        "                    maximum_momentum=0.90, minimum_momentum=0.80)\n",
        "callbacks = [checkpoint,lr_manager]\n",
        "model_info = model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=100,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "  2/360 [..............................] - ETA: 1:29:57 - loss: 11.1695 - gender_output_loss: 0.6632 - image_quality_output_loss: 1.0664 - age_output_loss: 1.7294 - weight_output_loss: 1.3792 - bag_output_loss: 1.5830 - footwear_output_loss: 1.6733 - pose_output_loss: 1.8239 - emotion_output_loss: 1.2512 - gender_output_acc: 0.6250 - image_quality_output_acc: 0.5000 - age_output_acc: 0.2188 - weight_output_acc: 0.3281 - bag_output_acc: 0.1875 - footwear_output_acc: 0.4062 - pose_output_acc: 0.1562 - emotion_output_acc: 0.4531"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.408874). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "360/360 [==============================] - 244s 678ms/step - loss: 8.6892 - gender_output_loss: 0.7585 - image_quality_output_loss: 1.0870 - age_output_loss: 1.5565 - weight_output_loss: 1.0780 - bag_output_loss: 1.0326 - footwear_output_loss: 1.1473 - pose_output_loss: 1.0275 - emotion_output_loss: 1.0018 - gender_output_acc: 0.5201 - image_quality_output_acc: 0.4938 - age_output_acc: 0.3357 - weight_output_acc: 0.6023 - bag_output_acc: 0.5020 - footwear_output_acc: 0.4039 - pose_output_acc: 0.5753 - emotion_output_acc: 0.7015 - val_loss: 8.5457 - val_gender_output_loss: 0.9483 - val_image_quality_output_loss: 1.0343 - val_age_output_loss: 1.5722 - val_weight_output_loss: 1.0039 - val_bag_output_loss: 1.0026 - val_footwear_output_loss: 1.0473 - val_pose_output_loss: 0.9393 - val_emotion_output_loss: 0.9979 - val_gender_output_acc: 0.5590 - val_image_quality_output_acc: 0.5645 - val_age_output_acc: 0.2455 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5570 - val_footwear_output_acc: 0.3715 - val_pose_output_acc: 0.6205 - val_emotion_output_acc: 0.7177\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 8.54566, saving model to /content/drive/My Drive/EIP/saved_models/assign5n_%s_model.001.h5\n",
            " - lr: 0.00140 - momentum: 0.90 \n",
            "Epoch 2/100\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.2609 - gender_output_loss: 0.7276 - image_quality_output_loss: 1.0370 - age_output_loss: 1.4985 - weight_output_loss: 1.0219 - bag_output_loss: 0.9773 - footwear_output_loss: 1.0904 - pose_output_loss: 0.9652 - emotion_output_loss: 0.9429 - gender_output_acc: 0.5201 - image_quality_output_acc: 0.5205 - age_output_acc: 0.3591 - weight_output_acc: 0.6321 - bag_output_acc: 0.5199 - footwear_output_acc: 0.4077 - pose_output_acc: 0.6115 - emotion_output_acc: 0.7105\n",
            "Epoch 00001: val_loss improved from inf to 8.54566, saving model to /content/drive/My Drive/EIP/saved_models/assign5n_%s_model.001.h5\n",
            "360/360 [==============================] - 208s 579ms/step - loss: 8.2623 - gender_output_loss: 0.7275 - image_quality_output_loss: 1.0369 - age_output_loss: 1.4984 - weight_output_loss: 1.0224 - bag_output_loss: 0.9779 - footwear_output_loss: 1.0903 - pose_output_loss: 0.9656 - emotion_output_loss: 0.9432 - gender_output_acc: 0.5204 - image_quality_output_acc: 0.5203 - age_output_acc: 0.3588 - weight_output_acc: 0.6317 - bag_output_acc: 0.5201 - footwear_output_acc: 0.4079 - pose_output_acc: 0.6110 - emotion_output_acc: 0.7103 - val_loss: 8.3946 - val_gender_output_loss: 0.7587 - val_image_quality_output_loss: 1.0105 - val_age_output_loss: 1.4846 - val_weight_output_loss: 1.0694 - val_bag_output_loss: 1.0559 - val_footwear_output_loss: 1.0854 - val_pose_output_loss: 1.0267 - val_emotion_output_loss: 0.9034 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5081 - val_age_output_acc: 0.2717 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5585 - val_footwear_output_acc: 0.4496 - val_pose_output_acc: 0.3836 - val_emotion_output_acc: 0.7218\n",
            "\n",
            "Epoch 00002: val_loss improved from 8.54566 to 8.39461, saving model to /content/drive/My Drive/EIP/saved_models/assign5n_%s_model.002.h5\n",
            " - lr: 0.00180 - momentum: 0.89 \n",
            "Epoch 3/100\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.2062 - gender_output_loss: 0.7278 - image_quality_output_loss: 1.0260 - age_output_loss: 1.4722 - weight_output_loss: 1.0188 - bag_output_loss: 0.9638 - footwear_output_loss: 1.0863 - pose_output_loss: 0.9664 - emotion_output_loss: 0.9448 - gender_output_acc: 0.5258 - image_quality_output_acc: 0.5251 - age_output_acc: 0.3724 - weight_output_acc: 0.6300 - bag_output_acc: 0.5220 - footwear_output_acc: 0.4195 - pose_output_acc: 0.6107 - emotion_output_acc: 0.7100\n",
            "Epoch 00002: val_loss improved from 8.54566 to 8.39461, saving model to /content/drive/My Drive/EIP/saved_models/assign5n_%s_model.002.h5\n",
            "360/360 [==============================] - 215s 596ms/step - loss: 8.2046 - gender_output_loss: 0.7277 - image_quality_output_loss: 1.0256 - age_output_loss: 1.4721 - weight_output_loss: 1.0183 - bag_output_loss: 0.9637 - footwear_output_loss: 1.0862 - pose_output_loss: 0.9665 - emotion_output_loss: 0.9445 - gender_output_acc: 0.5258 - image_quality_output_acc: 0.5255 - age_output_acc: 0.3723 - weight_output_acc: 0.6303 - bag_output_acc: 0.5217 - footwear_output_acc: 0.4194 - pose_output_acc: 0.6106 - emotion_output_acc: 0.7102 - val_loss: 7.9784 - val_gender_output_loss: 0.6972 - val_image_quality_output_loss: 0.9794 - val_age_output_loss: 1.4795 - val_weight_output_loss: 0.9811 - val_bag_output_loss: 0.9426 - val_footwear_output_loss: 1.0372 - val_pose_output_loss: 0.9280 - val_emotion_output_loss: 0.9332 - val_gender_output_acc: 0.4587 - val_image_quality_output_acc: 0.5610 - val_age_output_acc: 0.2480 - val_weight_output_acc: 0.6406 - val_bag_output_acc: 0.5590 - val_footwear_output_acc: 0.4284 - val_pose_output_acc: 0.6210 - val_emotion_output_acc: 0.7203\n",
            "\n",
            "Epoch 00003: val_loss improved from 8.39461 to 7.97835, saving model to /content/drive/My Drive/EIP/saved_models/assign5n_%s_model.003.h5\n",
            " - lr: 0.00220 - momentum: 0.89 \n",
            "Epoch 4/100\n",
            "360/360 [==============================] - 214s 593ms/step - loss: 8.1444 - gender_output_loss: 0.7127 - image_quality_output_loss: 1.0142 - age_output_loss: 1.4704 - weight_output_loss: 1.0128 - bag_output_loss: 0.9452 - footwear_output_loss: 1.0914 - pose_output_loss: 0.9533 - emotion_output_loss: 0.9445 - gender_output_acc: 0.5258 - image_quality_output_acc: 0.5328 - age_output_acc: 0.3660 - weight_output_acc: 0.6262 - bag_output_acc: 0.5383 - footwear_output_acc: 0.4144 - pose_output_acc: 0.6153 - emotion_output_acc: 0.7102 - val_loss: 8.2234 - val_gender_output_loss: 0.7806 - val_image_quality_output_loss: 0.9821 - val_age_output_loss: 1.4712 - val_weight_output_loss: 1.0147 - val_bag_output_loss: 0.9645 - val_footwear_output_loss: 1.1015 - val_pose_output_loss: 0.9906 - val_emotion_output_loss: 0.9184 - val_gender_output_acc: 0.5585 - val_image_quality_output_acc: 0.5655 - val_age_output_acc: 0.4017 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5554 - val_footwear_output_acc: 0.3715 - val_pose_output_acc: 0.6225 - val_emotion_output_acc: 0.7182\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 7.97835\n",
            " - lr: 0.00260 - momentum: 0.88 \n",
            "Epoch 5/100\n",
            "360/360 [==============================] - 213s 592ms/step - loss: 8.1013 - gender_output_loss: 0.7134 - image_quality_output_loss: 1.0102 - age_output_loss: 1.4602 - weight_output_loss: 1.0105 - bag_output_loss: 0.9432 - footwear_output_loss: 1.0762 - pose_output_loss: 0.9504 - emotion_output_loss: 0.9373 - gender_output_acc: 0.5279 - image_quality_output_acc: 0.5405 - age_output_acc: 0.3786 - weight_output_acc: 0.6301 - bag_output_acc: 0.5423 - footwear_output_acc: 0.4187 - pose_output_acc: 0.6158 - emotion_output_acc: 0.7102 - val_loss: 8.0803 - val_gender_output_loss: 0.6967 - val_image_quality_output_loss: 1.0752 - val_age_output_loss: 1.4366 - val_weight_output_loss: 1.0094 - val_bag_output_loss: 0.9345 - val_footwear_output_loss: 1.0738 - val_pose_output_loss: 0.9351 - val_emotion_output_loss: 0.9189 - val_gender_output_acc: 0.4390 - val_image_quality_output_acc: 0.2828 - val_age_output_acc: 0.4012 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5590 - val_footwear_output_acc: 0.3669 - val_pose_output_acc: 0.6200 - val_emotion_output_acc: 0.7208\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 7.97835\n",
            " - lr: 0.00300 - momentum: 0.88 \n",
            "Epoch 6/100\n",
            "360/360 [==============================] - 212s 588ms/step - loss: 8.0553 - gender_output_loss: 0.7022 - image_quality_output_loss: 1.0080 - age_output_loss: 1.4579 - weight_output_loss: 1.0001 - bag_output_loss: 0.9396 - footwear_output_loss: 1.0682 - pose_output_loss: 0.9526 - emotion_output_loss: 0.9266 - gender_output_acc: 0.5399 - image_quality_output_acc: 0.5407 - age_output_acc: 0.3727 - weight_output_acc: 0.6351 - bag_output_acc: 0.5442 - footwear_output_acc: 0.4181 - pose_output_acc: 0.6149 - emotion_output_acc: 0.7102 - val_loss: 8.0195 - val_gender_output_loss: 0.6891 - val_image_quality_output_loss: 0.9875 - val_age_output_loss: 1.4264 - val_weight_output_loss: 0.9870 - val_bag_output_loss: 0.9327 - val_footwear_output_loss: 1.1233 - val_pose_output_loss: 0.9686 - val_emotion_output_loss: 0.9050 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5630 - val_age_output_acc: 0.4032 - val_weight_output_acc: 0.6371 - val_bag_output_acc: 0.5595 - val_footwear_output_acc: 0.3674 - val_pose_output_acc: 0.6210 - val_emotion_output_acc: 0.7188\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 7.97835\n",
            " - lr: 0.00340 - momentum: 0.87 \n",
            "Epoch 7/100\n",
            "360/360 [==============================] - 211s 587ms/step - loss: 8.0516 - gender_output_loss: 0.7086 - image_quality_output_loss: 1.0104 - age_output_loss: 1.4536 - weight_output_loss: 1.0031 - bag_output_loss: 0.9355 - footwear_output_loss: 1.0647 - pose_output_loss: 0.9531 - emotion_output_loss: 0.9226 - gender_output_acc: 0.5252 - image_quality_output_acc: 0.5347 - age_output_acc: 0.3820 - weight_output_acc: 0.6299 - bag_output_acc: 0.5513 - footwear_output_acc: 0.4223 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7103 - val_loss: 7.9416 - val_gender_output_loss: 0.6908 - val_image_quality_output_loss: 0.9883 - val_age_output_loss: 1.4372 - val_weight_output_loss: 1.0051 - val_bag_output_loss: 0.9239 - val_footwear_output_loss: 1.0619 - val_pose_output_loss: 0.9346 - val_emotion_output_loss: 0.8998 - val_gender_output_acc: 0.5590 - val_image_quality_output_acc: 0.5610 - val_age_output_acc: 0.4037 - val_weight_output_acc: 0.6366 - val_bag_output_acc: 0.5539 - val_footwear_output_acc: 0.4506 - val_pose_output_acc: 0.6190 - val_emotion_output_acc: 0.7208\n",
            "\n",
            "Epoch 00007: val_loss improved from 7.97835 to 7.94155, saving model to /content/drive/My Drive/EIP/saved_models/assign5n_%s_model.007.h5\n",
            " - lr: 0.00380 - momentum: 0.87 \n",
            "Epoch 8/100\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.0195 - gender_output_loss: 0.7046 - image_quality_output_loss: 1.0024 - age_output_loss: 1.4505 - weight_output_loss: 1.0004 - bag_output_loss: 0.9344 - footwear_output_loss: 1.0586 - pose_output_loss: 0.9437 - emotion_output_loss: 0.9250 - gender_output_acc: 0.5340 - image_quality_output_acc: 0.5426 - age_output_acc: 0.3837 - weight_output_acc: 0.6329 - bag_output_acc: 0.5471 - footwear_output_acc: 0.4207 - pose_output_acc: 0.6175 - emotion_output_acc: 0.7103\n",
            "360/360 [==============================] - 210s 583ms/step - loss: 8.0199 - gender_output_loss: 0.7048 - image_quality_output_loss: 1.0025 - age_output_loss: 1.4507 - weight_output_loss: 0.9995 - bag_output_loss: 0.9347 - footwear_output_loss: 1.0586 - pose_output_loss: 0.9437 - emotion_output_loss: 0.9254 - gender_output_acc: 0.5332 - image_quality_output_acc: 0.5425 - age_output_acc: 0.3840 - weight_output_acc: 0.6334 - bag_output_acc: 0.5471 - footwear_output_acc: 0.4208 - pose_output_acc: 0.6175 - emotion_output_acc: 0.7101 - val_loss: 7.9165 - val_gender_output_loss: 0.6932 - val_image_quality_output_loss: 1.0050 - val_age_output_loss: 1.4383 - val_weight_output_loss: 0.9849 - val_bag_output_loss: 0.9262 - val_footwear_output_loss: 1.0429 - val_pose_output_loss: 0.9349 - val_emotion_output_loss: 0.8909 - val_gender_output_acc: 0.4909 - val_image_quality_output_acc: 0.5630 - val_age_output_acc: 0.4047 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5554 - val_footwear_output_acc: 0.4506 - val_pose_output_acc: 0.6190 - val_emotion_output_acc: 0.7193\n",
            "\n",
            "Epoch 00008: val_loss improved from 7.94155 to 7.91649, saving model to /content/drive/My Drive/EIP/saved_models/assign5n_%s_model.008.h5\n",
            " - lr: 0.00420 - momentum: 0.86 \n",
            "Epoch 9/100\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.0015 - gender_output_loss: 0.6990 - image_quality_output_loss: 1.0005 - age_output_loss: 1.4469 - weight_output_loss: 0.9989 - bag_output_loss: 0.9322 - footwear_output_loss: 1.0590 - pose_output_loss: 0.9413 - emotion_output_loss: 0.9237 - gender_output_acc: 0.5314 - image_quality_output_acc: 0.5476 - age_output_acc: 0.3893 - weight_output_acc: 0.6347 - bag_output_acc: 0.5514 - footwear_output_acc: 0.4183 - pose_output_acc: 0.6171 - emotion_output_acc: 0.7101\n",
            "Epoch 00008: val_loss improved from 7.94155 to 7.91649, saving model to /content/drive/My Drive/EIP/saved_models/assign5n_%s_model.008.h5\n",
            "360/360 [==============================] - 210s 583ms/step - loss: 8.0005 - gender_output_loss: 0.6990 - image_quality_output_loss: 1.0006 - age_output_loss: 1.4468 - weight_output_loss: 0.9985 - bag_output_loss: 0.9324 - footwear_output_loss: 1.0588 - pose_output_loss: 0.9408 - emotion_output_loss: 0.9235 - gender_output_acc: 0.5311 - image_quality_output_acc: 0.5472 - age_output_acc: 0.3890 - weight_output_acc: 0.6348 - bag_output_acc: 0.5513 - footwear_output_acc: 0.4183 - pose_output_acc: 0.6174 - emotion_output_acc: 0.7102 - val_loss: 7.9309 - val_gender_output_loss: 0.6985 - val_image_quality_output_loss: 0.9902 - val_age_output_loss: 1.4272 - val_weight_output_loss: 1.0139 - val_bag_output_loss: 0.9204 - val_footwear_output_loss: 1.0368 - val_pose_output_loss: 0.9309 - val_emotion_output_loss: 0.9130 - val_gender_output_acc: 0.4410 - val_image_quality_output_acc: 0.5610 - val_age_output_acc: 0.4022 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5544 - val_footwear_output_acc: 0.4521 - val_pose_output_acc: 0.6215 - val_emotion_output_acc: 0.7188\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 7.91649\n",
            " - lr: 0.00460 - momentum: 0.86 \n",
            "Epoch 10/100\n",
            "360/360 [==============================] - 212s 588ms/step - loss: 7.9893 - gender_output_loss: 0.6986 - image_quality_output_loss: 1.0006 - age_output_loss: 1.4485 - weight_output_loss: 0.9969 - bag_output_loss: 0.9270 - footwear_output_loss: 1.0579 - pose_output_loss: 0.9390 - emotion_output_loss: 0.9207 - gender_output_acc: 0.5322 - image_quality_output_acc: 0.5469 - age_output_acc: 0.3762 - weight_output_acc: 0.6347 - bag_output_acc: 0.5587 - footwear_output_acc: 0.4282 - pose_output_acc: 0.6174 - emotion_output_acc: 0.7102 - val_loss: 7.9741 - val_gender_output_loss: 0.6863 - val_image_quality_output_loss: 0.9973 - val_age_output_loss: 1.4591 - val_weight_output_loss: 1.0198 - val_bag_output_loss: 0.9211 - val_footwear_output_loss: 1.0475 - val_pose_output_loss: 0.9524 - val_emotion_output_loss: 0.8907 - val_gender_output_acc: 0.5590 - val_image_quality_output_acc: 0.5615 - val_age_output_acc: 0.4027 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5585 - val_footwear_output_acc: 0.4496 - val_pose_output_acc: 0.6174 - val_emotion_output_acc: 0.7218\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 7.91649\n",
            " - lr: 0.00500 - momentum: 0.86 \n",
            "Epoch 11/100\n",
            "360/360 [==============================] - 211s 586ms/step - loss: 7.9888 - gender_output_loss: 0.6990 - image_quality_output_loss: 1.0006 - age_output_loss: 1.4484 - weight_output_loss: 0.9948 - bag_output_loss: 0.9321 - footwear_output_loss: 1.0574 - pose_output_loss: 0.9374 - emotion_output_loss: 0.9190 - gender_output_acc: 0.5362 - image_quality_output_acc: 0.5484 - age_output_acc: 0.3853 - weight_output_acc: 0.6352 - bag_output_acc: 0.5516 - footwear_output_acc: 0.4189 - pose_output_acc: 0.6174 - emotion_output_acc: 0.7103 - val_loss: 7.9204 - val_gender_output_loss: 0.6965 - val_image_quality_output_loss: 0.9831 - val_age_output_loss: 1.4233 - val_weight_output_loss: 1.0075 - val_bag_output_loss: 0.9485 - val_footwear_output_loss: 1.0379 - val_pose_output_loss: 0.9341 - val_emotion_output_loss: 0.8894 - val_gender_output_acc: 0.4400 - val_image_quality_output_acc: 0.5680 - val_age_output_acc: 0.4017 - val_weight_output_acc: 0.6371 - val_bag_output_acc: 0.4677 - val_footwear_output_acc: 0.4491 - val_pose_output_acc: 0.6169 - val_emotion_output_acc: 0.7208\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 7.91649\n",
            " - lr: 0.00500 - momentum: 0.86 \n",
            "Epoch 11/100\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 7.91649\n",
            " - lr: 0.00540 - momentum: 0.85 \n",
            "Epoch 12/100\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.9775 - gender_output_loss: 0.7009 - image_quality_output_loss: 0.9950 - age_output_loss: 1.4449 - weight_output_loss: 0.9910 - bag_output_loss: 0.9316 - footwear_output_loss: 1.0552 - pose_output_loss: 0.9387 - emotion_output_loss: 0.9201 - gender_output_acc: 0.5295 - image_quality_output_acc: 0.5463 - age_output_acc: 0.3815 - weight_output_acc: 0.6353 - bag_output_acc: 0.5503 - footwear_output_acc: 0.4287 - pose_output_acc: 0.6170 - emotion_output_acc: 0.7104\n",
            "360/360 [==============================] - 212s 588ms/step - loss: 7.9776 - gender_output_loss: 0.7009 - image_quality_output_loss: 0.9947 - age_output_loss: 1.4449 - weight_output_loss: 0.9911 - bag_output_loss: 0.9319 - footwear_output_loss: 1.0553 - pose_output_loss: 0.9385 - emotion_output_loss: 0.9204 - gender_output_acc: 0.5294 - image_quality_output_acc: 0.5468 - age_output_acc: 0.3815 - weight_output_acc: 0.6352 - bag_output_acc: 0.5503 - footwear_output_acc: 0.4286 - pose_output_acc: 0.6171 - emotion_output_acc: 0.7102 - val_loss: 7.9034 - val_gender_output_loss: 0.6870 - val_image_quality_output_loss: 0.9803 - val_age_output_loss: 1.4510 - val_weight_output_loss: 0.9926 - val_bag_output_loss: 0.9188 - val_footwear_output_loss: 1.0456 - val_pose_output_loss: 0.9349 - val_emotion_output_loss: 0.8932 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5640 - val_age_output_acc: 0.4047 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5590 - val_footwear_output_acc: 0.4516 - val_pose_output_acc: 0.6179 - val_emotion_output_acc: 0.7203\n",
            "\n",
            "Epoch 00012: val_loss improved from 7.91649 to 7.90342, saving model to /content/drive/My Drive/EIP/saved_models/assign5n_%s_model.012.h5\n",
            " - lr: 0.00580 - momentum: 0.85 \n",
            "Epoch 13/100\n",
            "360/360 [==============================] - 210s 583ms/step - loss: 7.9705 - gender_output_loss: 0.6923 - image_quality_output_loss: 0.9951 - age_output_loss: 1.4429 - weight_output_loss: 0.9994 - bag_output_loss: 0.9263 - footwear_output_loss: 1.0545 - pose_output_loss: 0.9421 - emotion_output_loss: 0.9180 - gender_output_acc: 0.5445 - image_quality_output_acc: 0.5490 - age_output_acc: 0.3920 - weight_output_acc: 0.6316 - bag_output_acc: 0.5570 - footwear_output_acc: 0.4313 - pose_output_acc: 0.6170 - emotion_output_acc: 0.7102 - val_loss: 7.9024 - val_gender_output_loss: 0.7031 - val_image_quality_output_loss: 0.9771 - val_age_output_loss: 1.4432 - val_weight_output_loss: 0.9818 - val_bag_output_loss: 0.9216 - val_footwear_output_loss: 1.0556 - val_pose_output_loss: 0.9210 - val_emotion_output_loss: 0.8990 - val_gender_output_acc: 0.4385 - val_image_quality_output_acc: 0.5620 - val_age_output_acc: 0.3785 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.4521 - val_pose_output_acc: 0.6195 - val_emotion_output_acc: 0.7182\n",
            "\n",
            "Epoch 00013: val_loss improved from 7.90342 to 7.90244, saving model to /content/drive/My Drive/EIP/saved_models/assign5n_%s_model.013.h5\n",
            " - lr: 0.00620 - momentum: 0.84 \n",
            "Epoch 14/100\n",
            "360/360 [==============================] - 210s 583ms/step - loss: 7.9705 - gender_output_loss: 0.6923 - image_quality_output_loss: 0.9951 - age_output_loss: 1.4429 - weight_output_loss: 0.9994 - bag_output_loss: 0.9263 - footwear_output_loss: 1.0545 - pose_output_loss: 0.9421 - emotion_output_loss: 0.9180 - gender_output_acc: 0.5445 - image_quality_output_acc: 0.5490 - age_output_acc: 0.3920 - weight_output_acc: 0.6316 - bag_output_acc: 0.5570 - footwear_output_acc: 0.4313 - pose_output_acc: 0.6170 - emotion_output_acc: 0.7102 - val_loss: 7.9024 - val_gender_output_loss: 0.7031 - val_image_quality_output_loss: 0.9771 - val_age_output_loss: 1.4432 - val_weight_output_loss: 0.9818 - val_bag_output_loss: 0.9216 - val_footwear_output_loss: 1.0556 - val_pose_output_loss: 0.9210 - val_emotion_output_loss: 0.8990 - val_gender_output_acc: 0.4385 - val_image_quality_output_acc: 0.5620 - val_age_output_acc: 0.3785 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.4521 - val_pose_output_acc: 0.6195 - val_emotion_output_acc: 0.7182\n",
            "360/360 [==============================] - 211s 585ms/step - loss: 7.9701 - gender_output_loss: 0.6916 - image_quality_output_loss: 0.9995 - age_output_loss: 1.4454 - weight_output_loss: 0.9924 - bag_output_loss: 0.9281 - footwear_output_loss: 1.0557 - pose_output_loss: 0.9380 - emotion_output_loss: 0.9195 - gender_output_acc: 0.5461 - image_quality_output_acc: 0.5422 - age_output_acc: 0.3851 - weight_output_acc: 0.6349 - bag_output_acc: 0.5572 - footwear_output_acc: 0.4214 - pose_output_acc: 0.6171 - emotion_output_acc: 0.7102 - val_loss: 7.9011 - val_gender_output_loss: 0.6856 - val_image_quality_output_loss: 0.9973 - val_age_output_loss: 1.4424 - val_weight_output_loss: 0.9827 - val_bag_output_loss: 0.9198 - val_footwear_output_loss: 1.0380 - val_pose_output_loss: 0.9460 - val_emotion_output_loss: 0.8893 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5630 - val_age_output_acc: 0.4027 - val_weight_output_acc: 0.6416 - val_bag_output_acc: 0.5585 - val_footwear_output_acc: 0.4531 - val_pose_output_acc: 0.6205 - val_emotion_output_acc: 0.7233\n",
            "\n",
            "Epoch 00014: val_loss improved from 7.90244 to 7.90108, saving model to /content/drive/My Drive/EIP/saved_models/assign5n_%s_model.014.h5\n",
            " - lr: 0.00660 - momentum: 0.84 \n",
            "Epoch 15/100\n",
            "360/360 [==============================] - 213s 593ms/step - loss: 7.9595 - gender_output_loss: 0.6961 - image_quality_output_loss: 0.9954 - age_output_loss: 1.4440 - weight_output_loss: 0.9928 - bag_output_loss: 0.9251 - footwear_output_loss: 1.0537 - pose_output_loss: 0.9357 - emotion_output_loss: 0.9168 - gender_output_acc: 0.5387 - image_quality_output_acc: 0.5513 - age_output_acc: 0.3911 - weight_output_acc: 0.6348 - bag_output_acc: 0.5598 - footwear_output_acc: 0.4234 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7100 - val_loss: 7.9471 - val_gender_output_loss: 0.7309 - val_image_quality_output_loss: 0.9771 - val_age_output_loss: 1.4282 - val_weight_output_loss: 1.0052 - val_bag_output_loss: 0.9327 - val_footwear_output_loss: 1.0454 - val_pose_output_loss: 0.9333 - val_emotion_output_loss: 0.8944 - val_gender_output_acc: 0.5590 - val_image_quality_output_acc: 0.5625 - val_age_output_acc: 0.4027 - val_weight_output_acc: 0.6376 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.4491 - val_pose_output_acc: 0.6195 - val_emotion_output_acc: 0.7193\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 7.90108\n",
            " - lr: 0.00700 - momentum: 0.83 \n",
            "Epoch 16/100\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.9523 - gender_output_loss: 0.6921 - image_quality_output_loss: 0.9936 - age_output_loss: 1.4414 - weight_output_loss: 0.9926 - bag_output_loss: 0.9262 - footwear_output_loss: 1.0530 - pose_output_loss: 0.9380 - emotion_output_loss: 0.9154 - gender_output_acc: 0.5454 - image_quality_output_acc: 0.5514 - age_output_acc: 0.3917 - weight_output_acc: 0.6348 - bag_output_acc: 0.5562 - footwear_output_acc: 0.4266 - pose_output_acc: 0.6172 - emotion_output_acc: 0.7108\n",
            "Epoch 00015: val_loss did not improve from 7.90108\n",
            " - lr: 0.00700 - momentum: 0.83 \n",
            "360/360 [==============================] - 219s 609ms/step - loss: 7.9534 - gender_output_loss: 0.6921 - image_quality_output_loss: 0.9937 - age_output_loss: 1.4417 - weight_output_loss: 0.9927 - bag_output_loss: 0.9260 - footwear_output_loss: 1.0530 - pose_output_loss: 0.9379 - emotion_output_loss: 0.9164 - gender_output_acc: 0.5450 - image_quality_output_acc: 0.5511 - age_output_acc: 0.3911 - weight_output_acc: 0.6348 - bag_output_acc: 0.5566 - footwear_output_acc: 0.4264 - pose_output_acc: 0.6173 - emotion_output_acc: 0.7104 - val_loss: 7.8985 - val_gender_output_loss: 0.7031 - val_image_quality_output_loss: 0.9849 - val_age_output_loss: 1.4467 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9220 - val_footwear_output_loss: 1.0491 - val_pose_output_loss: 0.9284 - val_emotion_output_loss: 0.8858 - val_gender_output_acc: 0.4385 - val_image_quality_output_acc: 0.5670 - val_age_output_acc: 0.4047 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5585 - val_footwear_output_acc: 0.3654 - val_pose_output_acc: 0.6225 - val_emotion_output_acc: 0.7213\n",
            "\n",
            "Epoch 00016: val_loss improved from 7.90108 to 7.89851, saving model to /content/drive/My Drive/EIP/saved_models/assign5n_%s_model.016.h5\n",
            " - lr: 0.00740 - momentum: 0.83 \n",
            "Epoch 17/100\n",
            "360/360 [==============================] - 211s 585ms/step - loss: 7.9457 - gender_output_loss: 0.6944 - image_quality_output_loss: 0.9930 - age_output_loss: 1.4420 - weight_output_loss: 0.9898 - bag_output_loss: 0.9242 - footwear_output_loss: 1.0513 - pose_output_loss: 0.9350 - emotion_output_loss: 0.9160 - gender_output_acc: 0.5402 - image_quality_output_acc: 0.5513 - age_output_acc: 0.3894 - weight_output_acc: 0.6351 - bag_output_acc: 0.5614 - footwear_output_acc: 0.4251 - pose_output_acc: 0.6172 - emotion_output_acc: 0.7102 - val_loss: 7.8699 - val_gender_output_loss: 0.6858 - val_image_quality_output_loss: 0.9887 - val_age_output_loss: 1.4345 - val_weight_output_loss: 0.9807 - val_bag_output_loss: 0.9283 - val_footwear_output_loss: 1.0382 - val_pose_output_loss: 0.9204 - val_emotion_output_loss: 0.8933 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5665 - val_age_output_acc: 0.3987 - val_weight_output_acc: 0.6416 - val_bag_output_acc: 0.5575 - val_footwear_output_acc: 0.4516 - val_pose_output_acc: 0.6230 - val_emotion_output_acc: 0.7198\n",
            "\n",
            "Epoch 00017: val_loss improved from 7.89851 to 7.86986, saving model to /content/drive/My Drive/EIP/saved_models/assign5n_%s_model.017.h5\n",
            " - lr: 0.00780 - momentum: 0.82 \n",
            "Epoch 18/100\n",
            "360/360 [==============================] - 210s 584ms/step - loss: 7.9408 - gender_output_loss: 0.6940 - image_quality_output_loss: 0.9929 - age_output_loss: 1.4378 - weight_output_loss: 0.9898 - bag_output_loss: 0.9242 - footwear_output_loss: 1.0498 - pose_output_loss: 0.9359 - emotion_output_loss: 0.9164 - gender_output_acc: 0.5398 - image_quality_output_acc: 0.5502 - age_output_acc: 0.3955 - weight_output_acc: 0.6349 - bag_output_acc: 0.5615 - footwear_output_acc: 0.4251 - pose_output_acc: 0.6173 - emotion_output_acc: 0.7101 - val_loss: 7.8752 - val_gender_output_loss: 0.6865 - val_image_quality_output_loss: 0.9703 - val_age_output_loss: 1.4311 - val_weight_output_loss: 0.9834 - val_bag_output_loss: 0.9406 - val_footwear_output_loss: 1.0354 - val_pose_output_loss: 0.9330 - val_emotion_output_loss: 0.8949 - val_gender_output_acc: 0.5575 - val_image_quality_output_acc: 0.5635 - val_age_output_acc: 0.4042 - val_weight_output_acc: 0.6416 - val_bag_output_acc: 0.5585 - val_footwear_output_acc: 0.4526 - val_pose_output_acc: 0.6190 - val_emotion_output_acc: 0.7193\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 7.86986\n",
            " - lr: 0.00820 - momentum: 0.82 \n",
            "Epoch 19/100\n",
            "360/360 [==============================] - 211s 587ms/step - loss: 7.9352 - gender_output_loss: 0.6928 - image_quality_output_loss: 0.9901 - age_output_loss: 1.4376 - weight_output_loss: 0.9892 - bag_output_loss: 0.9235 - footwear_output_loss: 1.0498 - pose_output_loss: 0.9363 - emotion_output_loss: 0.9159 - gender_output_acc: 0.5444 - image_quality_output_acc: 0.5496 - age_output_acc: 0.3924 - weight_output_acc: 0.6349 - bag_output_acc: 0.5637 - footwear_output_acc: 0.4291 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7101 - val_loss: 7.8807 - val_gender_output_loss: 0.6861 - val_image_quality_output_loss: 0.9664 - val_age_output_loss: 1.4495 - val_weight_output_loss: 0.9799 - val_bag_output_loss: 0.9455 - val_footwear_output_loss: 1.0397 - val_pose_output_loss: 0.9276 - val_emotion_output_loss: 0.8861 - val_gender_output_acc: 0.5605 - val_image_quality_output_acc: 0.5660 - val_age_output_acc: 0.4022 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5575 - val_footwear_output_acc: 0.4511 - val_pose_output_acc: 0.6190 - val_emotion_output_acc: 0.7238\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 7.86986\n",
            " - lr: 0.00820 - momentum: 0.82 \n",
            "Epoch 19/100\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 7.86986\n",
            " - lr: 0.00860 - momentum: 0.82 \n",
            "Epoch 20/100\n",
            "360/360 [==============================] - 212s 588ms/step - loss: 7.9330 - gender_output_loss: 0.6922 - image_quality_output_loss: 0.9914 - age_output_loss: 1.4383 - weight_output_loss: 0.9907 - bag_output_loss: 0.9227 - footwear_output_loss: 1.0487 - pose_output_loss: 0.9340 - emotion_output_loss: 0.9151 - gender_output_acc: 0.5486 - image_quality_output_acc: 0.5509 - age_output_acc: 0.3959 - weight_output_acc: 0.6351 - bag_output_acc: 0.5625 - footwear_output_acc: 0.4302 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7099 - val_loss: 7.8747 - val_gender_output_loss: 0.6972 - val_image_quality_output_loss: 0.9738 - val_age_output_loss: 1.4308 - val_weight_output_loss: 0.9841 - val_bag_output_loss: 0.9319 - val_footwear_output_loss: 1.0364 - val_pose_output_loss: 0.9305 - val_emotion_output_loss: 0.8901 - val_gender_output_acc: 0.4415 - val_image_quality_output_acc: 0.5640 - val_age_output_acc: 0.4032 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.4526 - val_pose_output_acc: 0.6195 - val_emotion_output_acc: 0.7198\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 7.86986\n",
            "Epoch 00019: val_loss did not improve from 7.86986\n",
            " - lr: 0.00860 - momentum: 0.82 \n",
            "Epoch 20/100\n",
            "\n",
            " - lr: 0.00900 - momentum: 0.81 \n",
            "Epoch 21/100\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.9294 - gender_output_loss: 0.6910 - image_quality_output_loss: 0.9912 - age_output_loss: 1.4379 - weight_output_loss: 0.9901 - bag_output_loss: 0.9225 - footwear_output_loss: 1.0494 - pose_output_loss: 0.9327 - emotion_output_loss: 0.9146 - gender_output_acc: 0.5481 - image_quality_output_acc: 0.5514 - age_output_acc: 0.3947 - weight_output_acc: 0.6347 - bag_output_acc: 0.5649 - footwear_output_acc: 0.4281 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7105\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            " - lr: 0.00900 - momentum: 0.81 \n",
            "Epoch 21/100\n",
            "360/360 [==============================] - 212s 588ms/step - loss: 7.9289 - gender_output_loss: 0.6910 - image_quality_output_loss: 0.9909 - age_output_loss: 1.4377 - weight_output_loss: 0.9901 - bag_output_loss: 0.9223 - footwear_output_loss: 1.0495 - pose_output_loss: 0.9325 - emotion_output_loss: 0.9149 - gender_output_acc: 0.5482 - image_quality_output_acc: 0.5517 - age_output_acc: 0.3946 - weight_output_acc: 0.6347 - bag_output_acc: 0.5651 - footwear_output_acc: 0.4282 - pose_output_acc: 0.6170 - emotion_output_acc: 0.7103 - val_loss: 7.8694 - val_gender_output_loss: 0.6867 - val_image_quality_output_loss: 0.9699 - val_age_output_loss: 1.4252 - val_weight_output_loss: 0.9855 - val_bag_output_loss: 0.9294 - val_footwear_output_loss: 1.0387 - val_pose_output_loss: 0.9475 - val_emotion_output_loss: 0.8865 - val_gender_output_acc: 0.5585 - val_image_quality_output_acc: 0.5635 - val_age_output_acc: 0.4022 - val_weight_output_acc: 0.6386 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.4531 - val_pose_output_acc: 0.6164 - val_emotion_output_acc: 0.7228\n",
            "\n",
            "Epoch 00021: val_loss improved from 7.86986 to 7.86939, saving model to /content/drive/My Drive/EIP/saved_models/assign5n_%s_model.021.h5\n",
            " - lr: 0.00940 - momentum: 0.81 \n",
            "Epoch 22/100\n",
            "360/360 [==============================] - 209s 581ms/step - loss: 7.9242 - gender_output_loss: 0.6902 - image_quality_output_loss: 0.9904 - age_output_loss: 1.4376 - weight_output_loss: 0.9898 - bag_output_loss: 0.9223 - footwear_output_loss: 1.0466 - pose_output_loss: 0.9332 - emotion_output_loss: 0.9140 - gender_output_acc: 0.5507 - image_quality_output_acc: 0.5511 - age_output_acc: 0.3958 - weight_output_acc: 0.6351 - bag_output_acc: 0.5648 - footwear_output_acc: 0.4301 - pose_output_acc: 0.6174 - emotion_output_acc: 0.7104 - val_loss: 7.8651 - val_gender_output_loss: 0.6870 - val_image_quality_output_loss: 0.9769 - val_age_output_loss: 1.4292 - val_weight_output_loss: 0.9852 - val_bag_output_loss: 0.9266 - val_footwear_output_loss: 1.0453 - val_pose_output_loss: 0.9225 - val_emotion_output_loss: 0.8925 - val_gender_output_acc: 0.5600 - val_image_quality_output_acc: 0.5640 - val_age_output_acc: 0.4037 - val_weight_output_acc: 0.6421 - val_bag_output_acc: 0.5595 - val_footwear_output_acc: 0.4491 - val_pose_output_acc: 0.6184 - val_emotion_output_acc: 0.7188\n",
            "\n",
            "Epoch 00022: val_loss improved from 7.86939 to 7.86507, saving model to /content/drive/My Drive/EIP/saved_models/assign5n_%s_model.022.h5\n",
            " - lr: 0.00980 - momentum: 0.80 \n",
            "Epoch 23/100\n",
            "360/360 [==============================] - 209s 581ms/step - loss: 7.9252 - gender_output_loss: 0.6906 - image_quality_output_loss: 0.9913 - age_output_loss: 1.4344 - weight_output_loss: 0.9881 - bag_output_loss: 0.9212 - footwear_output_loss: 1.0494 - pose_output_loss: 0.9347 - emotion_output_loss: 0.9154 - gender_output_acc: 0.5422 - image_quality_output_acc: 0.5494 - age_output_acc: 0.3958 - weight_output_acc: 0.6352 - bag_output_acc: 0.5632 - footwear_output_acc: 0.4321 - pose_output_acc: 0.6174 - emotion_output_acc: 0.7100 - val_loss: 7.8532 - val_gender_output_loss: 0.6909 - val_image_quality_output_loss: 0.9688 - val_age_output_loss: 1.4270 - val_weight_output_loss: 0.9789 - val_bag_output_loss: 0.9326 - val_footwear_output_loss: 1.0479 - val_pose_output_loss: 0.9195 - val_emotion_output_loss: 0.8877 - val_gender_output_acc: 0.5595 - val_image_quality_output_acc: 0.5635 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5539 - val_footwear_output_acc: 0.3684 - val_pose_output_acc: 0.6230 - val_emotion_output_acc: 0.7213\n",
            "\n",
            "Epoch 00023: val_loss improved from 7.86507 to 7.85325, saving model to /content/drive/My Drive/EIP/saved_models/assign5n_%s_model.023.h5\n",
            " - lr: 0.00980 - momentum: 0.80 \n",
            "Epoch 24/100\n",
            "360/360 [==============================] - 208s 578ms/step - loss: 7.9182 - gender_output_loss: 0.6882 - image_quality_output_loss: 0.9902 - age_output_loss: 1.4347 - weight_output_loss: 0.9877 - bag_output_loss: 0.9218 - footwear_output_loss: 1.0473 - pose_output_loss: 0.9340 - emotion_output_loss: 0.9144 - gender_output_acc: 0.5528 - image_quality_output_acc: 0.5516 - age_output_acc: 0.3975 - weight_output_acc: 0.6350 - bag_output_acc: 0.5648 - footwear_output_acc: 0.4308 - pose_output_acc: 0.6174 - emotion_output_acc: 0.7102 - val_loss: 7.8600 - val_gender_output_loss: 0.7152 - val_image_quality_output_loss: 0.9663 - val_age_output_loss: 1.4329 - val_weight_output_loss: 0.9814 - val_bag_output_loss: 0.9208 - val_footwear_output_loss: 1.0375 - val_pose_output_loss: 0.9177 - val_emotion_output_loss: 0.8882 - val_gender_output_acc: 0.5595 - val_image_quality_output_acc: 0.5655 - val_age_output_acc: 0.4007 - val_weight_output_acc: 0.6376 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.4501 - val_pose_output_acc: 0.6220 - val_emotion_output_acc: 0.7208\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 7.85325\n",
            " - lr: 0.00940 - momentum: 0.81 \n",
            "Epoch 25/100\n",
            "360/360 [==============================] - 211s 585ms/step - loss: 7.9162 - gender_output_loss: 0.6898 - image_quality_output_loss: 0.9908 - age_output_loss: 1.4355 - weight_output_loss: 0.9866 - bag_output_loss: 0.9204 - footwear_output_loss: 1.0475 - pose_output_loss: 0.9325 - emotion_output_loss: 0.9130 - gender_output_acc: 0.5569 - image_quality_output_acc: 0.5513 - age_output_acc: 0.3978 - weight_output_acc: 0.6350 - bag_output_acc: 0.5641 - footwear_output_acc: 0.4303 - pose_output_acc: 0.6171 - emotion_output_acc: 0.7104 - val_loss: 7.8971 - val_gender_output_loss: 0.6901 - val_image_quality_output_loss: 0.9702 - val_age_output_loss: 1.4272 - val_weight_output_loss: 0.9857 - val_bag_output_loss: 0.9581 - val_footwear_output_loss: 1.0367 - val_pose_output_loss: 0.9211 - val_emotion_output_loss: 0.9079 - val_gender_output_acc: 0.5585 - val_image_quality_output_acc: 0.5635 - val_age_output_acc: 0.4027 - val_weight_output_acc: 0.6376 - val_bag_output_acc: 0.3453 - val_footwear_output_acc: 0.4511 - val_pose_output_acc: 0.6205 - val_emotion_output_acc: 0.7172\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 7.85325\n",
            " - lr: 0.00900 - momentum: 0.81 \n",
            "Epoch 26/100\n",
            "360/360 [==============================] - 211s 586ms/step - loss: 7.9095 - gender_output_loss: 0.6890 - image_quality_output_loss: 0.9891 - age_output_loss: 1.4345 - weight_output_loss: 0.9878 - bag_output_loss: 0.9191 - footwear_output_loss: 1.0469 - pose_output_loss: 0.9319 - emotion_output_loss: 0.9113 - gender_output_acc: 0.5522 - image_quality_output_acc: 0.5512 - age_output_acc: 0.3978 - weight_output_acc: 0.6347 - bag_output_acc: 0.5644 - footwear_output_acc: 0.4293 - pose_output_acc: 0.6170 - emotion_output_acc: 0.7108 - val_loss: 7.8790 - val_gender_output_loss: 0.6988 - val_image_quality_output_loss: 0.9751 - val_age_output_loss: 1.4227 - val_weight_output_loss: 0.9843 - val_bag_output_loss: 0.9252 - val_footwear_output_loss: 1.0603 - val_pose_output_loss: 0.9241 - val_emotion_output_loss: 0.8885 - val_gender_output_acc: 0.5575 - val_image_quality_output_acc: 0.5635 - val_age_output_acc: 0.4062 - val_weight_output_acc: 0.6371 - val_bag_output_acc: 0.5539 - val_footwear_output_acc: 0.3679 - val_pose_output_acc: 0.6190 - val_emotion_output_acc: 0.7203\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 7.85325\n",
            " - lr: 0.00860 - momentum: 0.82 \n",
            "Epoch 27/100\n",
            "360/360 [==============================] - 210s 585ms/step - loss: 7.9069 - gender_output_loss: 0.6893 - image_quality_output_loss: 0.9889 - age_output_loss: 1.4333 - weight_output_loss: 0.9867 - bag_output_loss: 0.9203 - footwear_output_loss: 1.0454 - pose_output_loss: 0.9313 - emotion_output_loss: 0.9116 - gender_output_acc: 0.5568 - image_quality_output_acc: 0.5513 - age_output_acc: 0.3975 - weight_output_acc: 0.6349 - bag_output_acc: 0.5648 - footwear_output_acc: 0.4274 - pose_output_acc: 0.6173 - emotion_output_acc: 0.7107 - val_loss: 7.8284 - val_gender_output_loss: 0.6866 - val_image_quality_output_loss: 0.9678 - val_age_output_loss: 1.4208 - val_weight_output_loss: 0.9849 - val_bag_output_loss: 0.9240 - val_footwear_output_loss: 1.0392 - val_pose_output_loss: 0.9202 - val_emotion_output_loss: 0.8849 - val_gender_output_acc: 0.5585 - val_image_quality_output_acc: 0.5645 - val_age_output_acc: 0.4078 - val_weight_output_acc: 0.6381 - val_bag_output_acc: 0.5559 - val_footwear_output_acc: 0.4536 - val_pose_output_acc: 0.6205 - val_emotion_output_acc: 0.7218\n",
            "\n",
            "Epoch 00027: val_loss improved from 7.85325 to 7.82842, saving model to /content/drive/My Drive/EIP/saved_models/assign5n_%s_model.027.h5\n",
            " - lr: 0.00820 - momentum: 0.82 \n",
            "Epoch 28/100\n",
            " 55/360 [===>..........................] - ETA: 3:00 - loss: 7.8917 - gender_output_loss: 0.6832 - image_quality_output_loss: 0.9941 - age_output_loss: 1.4328 - weight_output_loss: 0.9655 - bag_output_loss: 0.9288 - footwear_output_loss: 1.0364 - pose_output_loss: 0.9259 - emotion_output_loss: 0.9250 - gender_output_acc: 0.5778 - image_quality_output_acc: 0.5386 - age_output_acc: 0.3886 - weight_output_acc: 0.6580 - bag_output_acc: 0.5602 - footwear_output_acc: 0.4341 - pose_output_acc: 0.6239 - emotion_output_acc: 0.7051"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-336:\n",
            "Process ForkPoolWorker-331:\n",
            "Process ForkPoolWorker-333:\n",
            "Process ForkPoolWorker-332:\n",
            "Process ForkPoolWorker-327:\n",
            "Process ForkPoolWorker-335:\n",
            "Process ForkPoolWorker-330:\n",
            "Process ForkPoolWorker-334:\n",
            "Process ForkPoolWorker-329:\n",
            "Process ForkPoolWorker-328:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Process ForkPoolWorker-325:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 125, in worker\n",
            "    put((job, i, result))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 104, in __next__\n",
            "    return self.next(*args, **kwargs)\n",
            "Traceback (most recent call last):\n",
            "Process ForkPoolWorker-326:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 347, in put\n",
            "    self._writer.send_bytes(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"<ipython-input-12-6ea5f0577193>\", line 37, in __getitem__\n",
            "    image_f = next(self.generator.flow(x = image,y = None,batch_size =self.batch_size))\n",
            "  File \"<ipython-input-12-6ea5f0577193>\", line 37, in __getitem__\n",
            "    image_f = next(self.generator.flow(x = image,y = None,batch_size =self.batch_size))\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 398, in _send_bytes\n",
            "    self._send(buf)\n",
            "  File \"<ipython-input-12-6ea5f0577193>\", line 37, in __getitem__\n",
            "    image_f = next(self.generator.flow(x = image,y = None,batch_size =self.batch_size))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r360/360 [==============================]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 104, in __next__\n",
            "    return self.next(*args, **kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 104, in __next__\n",
            "    return self.next(*args, **kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"<ipython-input-12-6ea5f0577193>\", line 36, in __getitem__\n",
            "    self.generator.fit(image)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\", line 153, in _get_batches_of_transformed_samples\n",
            "    x.astype(self.dtype), params)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\", line 153, in _get_batches_of_transformed_samples\n",
            "    x.astype(self.dtype), params)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\", line 155, in _get_batches_of_transformed_samples\n",
            "    batch_x[i] = x\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"<ipython-input-12-6ea5f0577193>\", line 36, in __getitem__\n",
            "    self.generator.fit(image)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 870, in apply_transform\n",
            "    order=self.interpolation_order)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 967, in fit\n",
            "    x /= (self.std + 1e-6)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 870, in apply_transform\n",
            "    order=self.interpolation_order)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 334, in apply_affine_transform\n",
            "    x = np.stack(channel_images, axis=0)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 963, in fit\n",
            "    self.std = np.std(x, axis=(0, self.row_axis, self.col_axis))\n",
            "KeyboardInterrupt\n",
            "  File \"<__array_function__ internals>\", line 6, in std\n",
            "  File \"<__array_function__ internals>\", line 6, in stack\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/shape_base.py\", line 423, in stack\n",
            "    shapes = {arr.shape for arr in arrays}\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in apply_affine_transform\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in <listcomp>\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\", line 3381, in std\n",
            "    **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/shape_base.py\", line 423, in <setcomp>\n",
            "    shapes = {arr.shape for arr in arrays}\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 486, in affine_transform\n",
            "    output, order, mode, cval, None, None)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\", line 217, in _std\n",
            "    keepdims=keepdims)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\", line 183, in _var\n",
            "    arrmean = umr_sum(arr, axis, dtype, keepdims=True)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-2629bd435c28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwZKrx7HgM0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/drive/My Drive/EIP/saved_models/assign5n_%s_model.027.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frle-VWY40m9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "1447a97d-4f5e-4cc4-dedc-2402ec94c938"
      },
      "source": [
        "model.evaluate_generator(valid_gen)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7.840801669705298,\n",
              " 0.6865052811561092,\n",
              " 0.970879077911377,\n",
              " 1.4265184863921134,\n",
              " 0.9870480664314762,\n",
              " 0.921641182514929,\n",
              " 1.0383526759762918,\n",
              " 0.9237332017190995,\n",
              " 0.886123691835711,\n",
              " 0.5589717741935484,\n",
              " 0.5625,\n",
              " 0.4012096774193548,\n",
              " 0.6386088709677419,\n",
              " 0.5589717741935484,\n",
              " 0.4526209677419355,\n",
              " 0.6184475806451613,\n",
              " 0.7212701612903226]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcNfN02G5Y31",
        "colab_type": "text"
      },
      "source": [
        "Extra Model with Different LR Strategy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI1hJb4qM6OH",
        "colab_type": "code",
        "outputId": "c485c000-d388-42a0-95ff-45a128ce58d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#os.chdir('/content/gdrive/My Drive/EIP')\n",
        "save_dir = os.path.join('/content/gdrive/My Drive/EIP', 'saved_models')\n",
        "model_name = 'assign5_%s_model.{epoch:03d}.h5' \n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "opt = Adam(lr=lr_schedule(0))#SGD(lr=0.001, momentum=0.9)#Adam() #\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "model_info = model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate:  0.001\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 253s 702ms/step - loss: 8.2570 - gender_output_loss: 0.7207 - image_quality_output_loss: 1.0385 - age_output_loss: 1.5054 - weight_output_loss: 1.0302 - bag_output_loss: 0.9537 - footwear_output_loss: 1.0912 - pose_output_loss: 0.9678 - emotion_output_loss: 0.9496 - gender_output_acc: 0.5333 - image_quality_output_acc: 0.5203 - age_output_acc: 0.3623 - weight_output_acc: 0.6267 - bag_output_acc: 0.5378 - footwear_output_acc: 0.4131 - pose_output_acc: 0.6135 - emotion_output_acc: 0.7101 - val_loss: 8.0157 - val_gender_output_loss: 0.6868 - val_image_quality_output_loss: 0.9902 - val_age_output_loss: 1.4357 - val_weight_output_loss: 1.0078 - val_bag_output_loss: 0.9526 - val_footwear_output_loss: 1.0699 - val_pose_output_loss: 0.9652 - val_emotion_output_loss: 0.9075 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5534 - val_age_output_acc: 0.4113 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5554 - val_footwear_output_acc: 0.4526 - val_pose_output_acc: 0.6053 - val_emotion_output_acc: 0.7157\n",
            "Epoch 2/50\n",
            "Learning rate:  0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "360/360 [==============================] - 228s 633ms/step - loss: 8.0169 - gender_output_loss: 0.6960 - image_quality_output_loss: 0.9998 - age_output_loss: 1.4557 - weight_output_loss: 1.0003 - bag_output_loss: 0.9323 - footwear_output_loss: 1.0642 - pose_output_loss: 0.9355 - emotion_output_loss: 0.9332 - gender_output_acc: 0.5371 - image_quality_output_acc: 0.5467 - age_output_acc: 0.3843 - weight_output_acc: 0.6372 - bag_output_acc: 0.5476 - footwear_output_acc: 0.4157 - pose_output_acc: 0.6196 - emotion_output_acc: 0.7109 - val_loss: 7.9646 - val_gender_output_loss: 0.6906 - val_image_quality_output_loss: 0.9953 - val_age_output_loss: 1.4489 - val_weight_output_loss: 1.0043 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 1.0300 - val_pose_output_loss: 0.9510 - val_emotion_output_loss: 0.9086 - val_gender_output_acc: 0.5605 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.4168 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5554 - val_footwear_output_acc: 0.4582 - val_pose_output_acc: 0.6033 - val_emotion_output_acc: 0.7142\n",
            "Epoch 3/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 229s 636ms/step - loss: 8.0090 - gender_output_loss: 0.6984 - image_quality_output_loss: 0.9980 - age_output_loss: 1.4533 - weight_output_loss: 1.0013 - bag_output_loss: 0.9303 - footwear_output_loss: 1.0593 - pose_output_loss: 0.9449 - emotion_output_loss: 0.9235 - gender_output_acc: 0.5336 - image_quality_output_acc: 0.5482 - age_output_acc: 0.3821 - weight_output_acc: 0.6373 - bag_output_acc: 0.5582 - footwear_output_acc: 0.4193 - pose_output_acc: 0.6198 - emotion_output_acc: 0.7112 - val_loss: 8.0369 - val_gender_output_loss: 0.6884 - val_image_quality_output_loss: 0.9955 - val_age_output_loss: 1.4154 - val_weight_output_loss: 0.9983 - val_bag_output_loss: 0.9701 - val_footwear_output_loss: 1.0420 - val_pose_output_loss: 1.0044 - val_emotion_output_loss: 0.9228 - val_gender_output_acc: 0.5585 - val_image_quality_output_acc: 0.5575 - val_age_output_acc: 0.4143 - val_weight_output_acc: 0.6260 - val_bag_output_acc: 0.5529 - val_footwear_output_acc: 0.4561 - val_pose_output_acc: 0.6048 - val_emotion_output_acc: 0.7157\n",
            "Epoch 4/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 229s 636ms/step - loss: 8.0023 - gender_output_loss: 0.6985 - image_quality_output_loss: 1.0040 - age_output_loss: 1.4525 - weight_output_loss: 0.9988 - bag_output_loss: 0.9296 - footwear_output_loss: 1.0575 - pose_output_loss: 0.9382 - emotion_output_loss: 0.9232 - gender_output_acc: 0.5365 - image_quality_output_acc: 0.5457 - age_output_acc: 0.3871 - weight_output_acc: 0.6373 - bag_output_acc: 0.5561 - footwear_output_acc: 0.4220 - pose_output_acc: 0.6197 - emotion_output_acc: 0.7113 - val_loss: 7.9742 - val_gender_output_loss: 0.7005 - val_image_quality_output_loss: 1.0003 - val_age_output_loss: 1.4258 - val_weight_output_loss: 1.0030 - val_bag_output_loss: 0.9522 - val_footwear_output_loss: 1.0348 - val_pose_output_loss: 0.9568 - val_emotion_output_loss: 0.9008 - val_gender_output_acc: 0.5600 - val_image_quality_output_acc: 0.5549 - val_age_output_acc: 0.4128 - val_weight_output_acc: 0.6285 - val_bag_output_acc: 0.5549 - val_footwear_output_acc: 0.4567 - val_pose_output_acc: 0.6053 - val_emotion_output_acc: 0.7142\n",
            "Epoch 5/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 230s 638ms/step - loss: 7.9927 - gender_output_loss: 0.6933 - image_quality_output_loss: 0.9978 - age_output_loss: 1.4570 - weight_output_loss: 0.9958 - bag_output_loss: 0.9269 - footwear_output_loss: 1.0555 - pose_output_loss: 0.9427 - emotion_output_loss: 0.9238 - gender_output_acc: 0.5463 - image_quality_output_acc: 0.5480 - age_output_acc: 0.3815 - weight_output_acc: 0.6376 - bag_output_acc: 0.5601 - footwear_output_acc: 0.4227 - pose_output_acc: 0.6196 - emotion_output_acc: 0.7111 - val_loss: 7.9163 - val_gender_output_loss: 0.6979 - val_image_quality_output_loss: 0.9889 - val_age_output_loss: 1.4109 - val_weight_output_loss: 0.9957 - val_bag_output_loss: 0.9427 - val_footwear_output_loss: 1.0285 - val_pose_output_loss: 0.9391 - val_emotion_output_loss: 0.9127 - val_gender_output_acc: 0.4350 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.4189 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5529 - val_footwear_output_acc: 0.4587 - val_pose_output_acc: 0.6074 - val_emotion_output_acc: 0.7117\n",
            "Epoch 6/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 230s 638ms/step - loss: 8.0069 - gender_output_loss: 0.6984 - image_quality_output_loss: 1.0001 - age_output_loss: 1.4535 - weight_output_loss: 0.9984 - bag_output_loss: 0.9309 - footwear_output_loss: 1.0595 - pose_output_loss: 0.9382 - emotion_output_loss: 0.9277 - gender_output_acc: 0.5395 - image_quality_output_acc: 0.5418 - age_output_acc: 0.3788 - weight_output_acc: 0.6372 - bag_output_acc: 0.5512 - footwear_output_acc: 0.4148 - pose_output_acc: 0.6198 - emotion_output_acc: 0.7111 - val_loss: 8.0513 - val_gender_output_loss: 0.6865 - val_image_quality_output_loss: 1.0037 - val_age_output_loss: 1.4376 - val_weight_output_loss: 0.9931 - val_bag_output_loss: 0.9387 - val_footwear_output_loss: 1.0342 - val_pose_output_loss: 1.0473 - val_emotion_output_loss: 0.9103 - val_gender_output_acc: 0.5595 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.4158 - val_weight_output_acc: 0.6265 - val_bag_output_acc: 0.5534 - val_footwear_output_acc: 0.4567 - val_pose_output_acc: 0.2293 - val_emotion_output_acc: 0.7127\n",
            "Epoch 7/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 229s 636ms/step - loss: 7.9911 - gender_output_loss: 0.6956 - image_quality_output_loss: 0.9998 - age_output_loss: 1.4482 - weight_output_loss: 0.9961 - bag_output_loss: 0.9289 - footwear_output_loss: 1.0537 - pose_output_loss: 0.9428 - emotion_output_loss: 0.9259 - gender_output_acc: 0.5331 - image_quality_output_acc: 0.5494 - age_output_acc: 0.3859 - weight_output_acc: 0.6376 - bag_output_acc: 0.5485 - footwear_output_acc: 0.4230 - pose_output_acc: 0.6189 - emotion_output_acc: 0.7114 - val_loss: 8.0023 - val_gender_output_loss: 0.6905 - val_image_quality_output_loss: 0.9922 - val_age_output_loss: 1.4291 - val_weight_output_loss: 1.0082 - val_bag_output_loss: 0.9409 - val_footwear_output_loss: 1.0781 - val_pose_output_loss: 0.9413 - val_emotion_output_loss: 0.9220 - val_gender_output_acc: 0.5580 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.4143 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5544 - val_footwear_output_acc: 0.3684 - val_pose_output_acc: 0.6053 - val_emotion_output_acc: 0.7117\n",
            "Epoch 8/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 229s 637ms/step - loss: 7.9673 - gender_output_loss: 0.6939 - image_quality_output_loss: 0.9921 - age_output_loss: 1.4463 - weight_output_loss: 0.9972 - bag_output_loss: 0.9253 - footwear_output_loss: 1.0567 - pose_output_loss: 0.9337 - emotion_output_loss: 0.9219 - gender_output_acc: 0.5462 - image_quality_output_acc: 0.5527 - age_output_acc: 0.3917 - weight_output_acc: 0.6372 - bag_output_acc: 0.5618 - footwear_output_acc: 0.4232 - pose_output_acc: 0.6198 - emotion_output_acc: 0.7110 - val_loss: 7.9656 - val_gender_output_loss: 0.6880 - val_image_quality_output_loss: 1.0199 - val_age_output_loss: 1.4361 - val_weight_output_loss: 0.9847 - val_bag_output_loss: 0.9481 - val_footwear_output_loss: 1.0387 - val_pose_output_loss: 0.9459 - val_emotion_output_loss: 0.9042 - val_gender_output_acc: 0.5595 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6310 - val_bag_output_acc: 0.5549 - val_footwear_output_acc: 0.4551 - val_pose_output_acc: 0.6013 - val_emotion_output_acc: 0.7167\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 9/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 231s 642ms/step - loss: 7.9768 - gender_output_loss: 0.6943 - image_quality_output_loss: 0.9962 - age_output_loss: 1.4542 - weight_output_loss: 0.9979 - bag_output_loss: 0.9235 - footwear_output_loss: 1.0522 - pose_output_loss: 0.9390 - emotion_output_loss: 0.9194 - gender_output_acc: 0.5453 - image_quality_output_acc: 0.5498 - age_output_acc: 0.3831 - weight_output_acc: 0.6372 - bag_output_acc: 0.5605 - footwear_output_acc: 0.4251 - pose_output_acc: 0.6198 - emotion_output_acc: 0.7114 - val_loss: 8.4601 - val_gender_output_loss: 0.7176 - val_image_quality_output_loss: 1.0008 - val_age_output_loss: 1.5227 - val_weight_output_loss: 1.0378 - val_bag_output_loss: 1.1004 - val_footwear_output_loss: 1.0756 - val_pose_output_loss: 0.9574 - val_emotion_output_loss: 1.0477 - val_gender_output_acc: 0.5600 - val_image_quality_output_acc: 0.5544 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6290 - val_bag_output_acc: 0.5554 - val_footwear_output_acc: 0.4556 - val_pose_output_acc: 0.6079 - val_emotion_output_acc: 0.7137\n",
            "Epoch 10/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 231s 642ms/step - loss: 7.9753 - gender_output_loss: 0.6933 - image_quality_output_loss: 0.9955 - age_output_loss: 1.4509 - weight_output_loss: 0.9966 - bag_output_loss: 0.9252 - footwear_output_loss: 1.0569 - pose_output_loss: 0.9325 - emotion_output_loss: 0.9245 - gender_output_acc: 0.5418 - image_quality_output_acc: 0.5519 - age_output_acc: 0.3861 - weight_output_acc: 0.6372 - bag_output_acc: 0.5556 - footwear_output_acc: 0.4253 - pose_output_acc: 0.6194 - emotion_output_acc: 0.7111 - val_loss: 8.0095 - val_gender_output_loss: 0.6862 - val_image_quality_output_loss: 1.0117 - val_age_output_loss: 1.4216 - val_weight_output_loss: 1.0108 - val_bag_output_loss: 0.9378 - val_footwear_output_loss: 1.0946 - val_pose_output_loss: 0.9397 - val_emotion_output_loss: 0.9069 - val_gender_output_acc: 0.5585 - val_image_quality_output_acc: 0.5529 - val_age_output_acc: 0.4158 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5529 - val_footwear_output_acc: 0.4516 - val_pose_output_acc: 0.6089 - val_emotion_output_acc: 0.7137\n",
            "Epoch 11/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 231s 642ms/step - loss: 7.9722 - gender_output_loss: 0.6987 - image_quality_output_loss: 0.9965 - age_output_loss: 1.4525 - weight_output_loss: 0.9939 - bag_output_loss: 0.9257 - footwear_output_loss: 1.0535 - pose_output_loss: 0.9343 - emotion_output_loss: 0.9170 - gender_output_acc: 0.5282 - image_quality_output_acc: 0.5523 - age_output_acc: 0.3827 - weight_output_acc: 0.6376 - bag_output_acc: 0.5590 - footwear_output_acc: 0.4269 - pose_output_acc: 0.6198 - emotion_output_acc: 0.7114 - val_loss: 7.9496 - val_gender_output_loss: 0.6907 - val_image_quality_output_loss: 0.9767 - val_age_output_loss: 1.4121 - val_weight_output_loss: 1.0125 - val_bag_output_loss: 0.9454 - val_footwear_output_loss: 1.0560 - val_pose_output_loss: 0.9542 - val_emotion_output_loss: 0.9019 - val_gender_output_acc: 0.5595 - val_image_quality_output_acc: 0.5549 - val_age_output_acc: 0.4153 - val_weight_output_acc: 0.6275 - val_bag_output_acc: 0.5534 - val_footwear_output_acc: 0.4536 - val_pose_output_acc: 0.6079 - val_emotion_output_acc: 0.7162\n",
            "Epoch 12/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 231s 642ms/step - loss: 7.9677 - gender_output_loss: 0.6947 - image_quality_output_loss: 0.9949 - age_output_loss: 1.4484 - weight_output_loss: 0.9992 - bag_output_loss: 0.9231 - footwear_output_loss: 1.0513 - pose_output_loss: 0.9337 - emotion_output_loss: 0.9224 - gender_output_acc: 0.5383 - image_quality_output_acc: 0.5506 - age_output_acc: 0.3850 - weight_output_acc: 0.6373 - bag_output_acc: 0.5643 - footwear_output_acc: 0.4242 - pose_output_acc: 0.6197 - emotion_output_acc: 0.7113 - val_loss: 8.2225 - val_gender_output_loss: 0.6859 - val_image_quality_output_loss: 1.0155 - val_age_output_loss: 1.4397 - val_weight_output_loss: 1.1278 - val_bag_output_loss: 0.9609 - val_footwear_output_loss: 1.0753 - val_pose_output_loss: 1.0127 - val_emotion_output_loss: 0.9047 - val_gender_output_acc: 0.5605 - val_image_quality_output_acc: 0.5534 - val_age_output_acc: 0.4113 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5484 - val_footwear_output_acc: 0.3720 - val_pose_output_acc: 0.6048 - val_emotion_output_acc: 0.7167\n",
            "Epoch 13/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 231s 642ms/step - loss: 7.9481 - gender_output_loss: 0.6928 - image_quality_output_loss: 0.9920 - age_output_loss: 1.4463 - weight_output_loss: 0.9908 - bag_output_loss: 0.9244 - footwear_output_loss: 1.0510 - pose_output_loss: 0.9339 - emotion_output_loss: 0.9168 - gender_output_acc: 0.5402 - image_quality_output_acc: 0.5526 - age_output_acc: 0.3924 - weight_output_acc: 0.6372 - bag_output_acc: 0.5630 - footwear_output_acc: 0.4221 - pose_output_acc: 0.6200 - emotion_output_acc: 0.7111 - val_loss: 7.9219 - val_gender_output_loss: 0.6875 - val_image_quality_output_loss: 0.9810 - val_age_output_loss: 1.4164 - val_weight_output_loss: 0.9991 - val_bag_output_loss: 0.9333 - val_footwear_output_loss: 1.0357 - val_pose_output_loss: 0.9598 - val_emotion_output_loss: 0.9091 - val_gender_output_acc: 0.5585 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.4148 - val_weight_output_acc: 0.6260 - val_bag_output_acc: 0.5539 - val_footwear_output_acc: 0.4541 - val_pose_output_acc: 0.6043 - val_emotion_output_acc: 0.7157\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 14/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 232s 644ms/step - loss: 7.9553 - gender_output_loss: 0.6926 - image_quality_output_loss: 0.9910 - age_output_loss: 1.4472 - weight_output_loss: 0.9936 - bag_output_loss: 0.9241 - footwear_output_loss: 1.0526 - pose_output_loss: 0.9337 - emotion_output_loss: 0.9206 - gender_output_acc: 0.5455 - image_quality_output_acc: 0.5528 - age_output_acc: 0.3860 - weight_output_acc: 0.6377 - bag_output_acc: 0.5616 - footwear_output_acc: 0.4240 - pose_output_acc: 0.6194 - emotion_output_acc: 0.7110 - val_loss: 8.0328 - val_gender_output_loss: 0.7093 - val_image_quality_output_loss: 1.0066 - val_age_output_loss: 1.4054 - val_weight_output_loss: 1.0438 - val_bag_output_loss: 0.9792 - val_footwear_output_loss: 1.0343 - val_pose_output_loss: 0.9466 - val_emotion_output_loss: 0.9075 - val_gender_output_acc: 0.4400 - val_image_quality_output_acc: 0.5600 - val_age_output_acc: 0.4168 - val_weight_output_acc: 0.6250 - val_bag_output_acc: 0.5539 - val_footwear_output_acc: 0.4561 - val_pose_output_acc: 0.6079 - val_emotion_output_acc: 0.7157\n",
            "Epoch 15/50\n",
            "Learning rate:  0.001\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.9488 - gender_output_loss: 0.6904 - image_quality_output_loss: 0.9970 - age_output_loss: 1.4456 - weight_output_loss: 0.9911 - bag_output_loss: 0.9236 - footwear_output_loss: 1.0511 - pose_output_loss: 0.9328 - emotion_output_loss: 0.9173 - gender_output_acc: 0.5455 - image_quality_output_acc: 0.5501 - age_output_acc: 0.3839 - weight_output_acc: 0.6377 - bag_output_acc: 0.5600 - footwear_output_acc: 0.4310 - pose_output_acc: 0.6194 - emotion_output_acc: 0.7113Learning rate:  0.001\n",
            "360/360 [==============================] - 232s 645ms/step - loss: 7.9494 - gender_output_loss: 0.6904 - image_quality_output_loss: 0.9970 - age_output_loss: 1.4455 - weight_output_loss: 0.9913 - bag_output_loss: 0.9236 - footwear_output_loss: 1.0511 - pose_output_loss: 0.9325 - emotion_output_loss: 0.9181 - gender_output_acc: 0.5454 - image_quality_output_acc: 0.5500 - age_output_acc: 0.3839 - weight_output_acc: 0.6377 - bag_output_acc: 0.5598 - footwear_output_acc: 0.4310 - pose_output_acc: 0.6195 - emotion_output_acc: 0.7109 - val_loss: 7.9624 - val_gender_output_loss: 0.6877 - val_image_quality_output_loss: 0.9793 - val_age_output_loss: 1.4130 - val_weight_output_loss: 1.0347 - val_bag_output_loss: 0.9322 - val_footwear_output_loss: 1.0576 - val_pose_output_loss: 0.9566 - val_emotion_output_loss: 0.9014 - val_gender_output_acc: 0.5595 - val_image_quality_output_acc: 0.5544 - val_age_output_acc: 0.4153 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5559 - val_footwear_output_acc: 0.3700 - val_pose_output_acc: 0.6038 - val_emotion_output_acc: 0.7132\n",
            "Epoch 16/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 231s 643ms/step - loss: 7.9486 - gender_output_loss: 0.6908 - image_quality_output_loss: 0.9906 - age_output_loss: 1.4455 - weight_output_loss: 0.9895 - bag_output_loss: 0.9264 - footwear_output_loss: 1.0577 - pose_output_loss: 0.9305 - emotion_output_loss: 0.9177 - gender_output_acc: 0.5479 - image_quality_output_acc: 0.5526 - age_output_acc: 0.3842 - weight_output_acc: 0.6372 - bag_output_acc: 0.5594 - footwear_output_acc: 0.4168 - pose_output_acc: 0.6200 - emotion_output_acc: 0.7110 - val_loss: 7.9124 - val_gender_output_loss: 0.6858 - val_image_quality_output_loss: 0.9769 - val_age_output_loss: 1.4225 - val_weight_output_loss: 0.9933 - val_bag_output_loss: 0.9500 - val_footwear_output_loss: 1.0332 - val_pose_output_loss: 0.9469 - val_emotion_output_loss: 0.9038 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.4183 - val_weight_output_acc: 0.6280 - val_bag_output_acc: 0.5534 - val_footwear_output_acc: 0.4567 - val_pose_output_acc: 0.6058 - val_emotion_output_acc: 0.7177\n",
            "Epoch 17/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 231s 643ms/step - loss: 7.9437 - gender_output_loss: 0.6902 - image_quality_output_loss: 0.9925 - age_output_loss: 1.4476 - weight_output_loss: 0.9906 - bag_output_loss: 0.9204 - footwear_output_loss: 1.0505 - pose_output_loss: 0.9338 - emotion_output_loss: 0.9180 - gender_output_acc: 0.5505 - image_quality_output_acc: 0.5529 - age_output_acc: 0.3855 - weight_output_acc: 0.6375 - bag_output_acc: 0.5605 - footwear_output_acc: 0.4281 - pose_output_acc: 0.6200 - emotion_output_acc: 0.7109 - val_loss: 7.9697 - val_gender_output_loss: 0.6992 - val_image_quality_output_loss: 0.9973 - val_age_output_loss: 1.4355 - val_weight_output_loss: 0.9976 - val_bag_output_loss: 0.9498 - val_footwear_output_loss: 1.0317 - val_pose_output_loss: 0.9513 - val_emotion_output_loss: 0.9073 - val_gender_output_acc: 0.5590 - val_image_quality_output_acc: 0.5575 - val_age_output_acc: 0.4189 - val_weight_output_acc: 0.6270 - val_bag_output_acc: 0.5570 - val_footwear_output_acc: 0.4567 - val_pose_output_acc: 0.6058 - val_emotion_output_acc: 0.7172\n",
            "Epoch 18/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 231s 642ms/step - loss: 7.9453 - gender_output_loss: 0.6925 - image_quality_output_loss: 0.9922 - age_output_loss: 1.4464 - weight_output_loss: 0.9897 - bag_output_loss: 0.9235 - footwear_output_loss: 1.0551 - pose_output_loss: 0.9296 - emotion_output_loss: 0.9164 - gender_output_acc: 0.5484 - image_quality_output_acc: 0.5497 - age_output_acc: 0.3895 - weight_output_acc: 0.6377 - bag_output_acc: 0.5607 - footwear_output_acc: 0.4168 - pose_output_acc: 0.6200 - emotion_output_acc: 0.7112 - val_loss: 11.4242 - val_gender_output_loss: 0.6917 - val_image_quality_output_loss: 1.3678 - val_age_output_loss: 1.8728 - val_weight_output_loss: 1.8045 - val_bag_output_loss: 1.1295 - val_footwear_output_loss: 1.2847 - val_pose_output_loss: 1.3557 - val_emotion_output_loss: 1.9173 - val_gender_output_acc: 0.5071 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.4148 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.5519 - val_footwear_output_acc: 0.4556 - val_pose_output_acc: 0.6043 - val_emotion_output_acc: 0.7147\n",
            "Epoch 19/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 231s 642ms/step - loss: 7.9395 - gender_output_loss: 0.6921 - image_quality_output_loss: 0.9888 - age_output_loss: 1.4452 - weight_output_loss: 0.9905 - bag_output_loss: 0.9210 - footwear_output_loss: 1.0564 - pose_output_loss: 0.9325 - emotion_output_loss: 0.9131 - gender_output_acc: 0.5461 - image_quality_output_acc: 0.5530 - age_output_acc: 0.3947 - weight_output_acc: 0.6375 - bag_output_acc: 0.5652 - footwear_output_acc: 0.4261 - pose_output_acc: 0.6197 - emotion_output_acc: 0.7112 - val_loss: 7.9981 - val_gender_output_loss: 0.6883 - val_image_quality_output_loss: 0.9950 - val_age_output_loss: 1.4277 - val_weight_output_loss: 1.0007 - val_bag_output_loss: 0.9344 - val_footwear_output_loss: 1.0317 - val_pose_output_loss: 1.0081 - val_emotion_output_loss: 0.9123 - val_gender_output_acc: 0.5575 - val_image_quality_output_acc: 0.5544 - val_age_output_acc: 0.4153 - val_weight_output_acc: 0.6250 - val_bag_output_acc: 0.5534 - val_footwear_output_acc: 0.4572 - val_pose_output_acc: 0.6069 - val_emotion_output_acc: 0.7188\n",
            "Epoch 20/50\n",
            "Learning rate:  0.001\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.9266 - gender_output_loss: 0.6913 - image_quality_output_loss: 0.9909 - age_output_loss: 1.4428 - weight_output_loss: 0.9860 - bag_output_loss: 0.9213 - footwear_output_loss: 1.0494 - pose_output_loss: 0.9308 - emotion_output_loss: 0.9140 - gender_output_acc: 0.5491 - image_quality_output_acc: 0.5533 - age_output_acc: 0.3883 - weight_output_acc: 0.6378 - bag_output_acc: 0.5649 - footwear_output_acc: 0.4278 - pose_output_acc: 0.6197 - emotion_output_acc: 0.7111Epoch 20/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 231s 640ms/step - loss: 7.9265 - gender_output_loss: 0.6912 - image_quality_output_loss: 0.9912 - age_output_loss: 1.4426 - weight_output_loss: 0.9866 - bag_output_loss: 0.9211 - footwear_output_loss: 1.0491 - pose_output_loss: 0.9309 - emotion_output_loss: 0.9137 - gender_output_acc: 0.5493 - image_quality_output_acc: 0.5530 - age_output_acc: 0.3886 - weight_output_acc: 0.6374 - bag_output_acc: 0.5651 - footwear_output_acc: 0.4281 - pose_output_acc: 0.6196 - emotion_output_acc: 0.7112 - val_loss: 8.1280 - val_gender_output_loss: 0.7012 - val_image_quality_output_loss: 0.9761 - val_age_output_loss: 1.4465 - val_weight_output_loss: 1.0280 - val_bag_output_loss: 0.9542 - val_footwear_output_loss: 1.0528 - val_pose_output_loss: 0.9604 - val_emotion_output_loss: 1.0086 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5590 - val_age_output_acc: 0.4148 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.5580 - val_footwear_output_acc: 0.4556 - val_pose_output_acc: 0.6028 - val_emotion_output_acc: 0.7167\n",
            "Epoch 21/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 231s 642ms/step - loss: 7.9313 - gender_output_loss: 0.6915 - image_quality_output_loss: 0.9913 - age_output_loss: 1.4415 - weight_output_loss: 0.9882 - bag_output_loss: 0.9208 - footwear_output_loss: 1.0509 - pose_output_loss: 0.9312 - emotion_output_loss: 0.9159 - gender_output_acc: 0.5471 - image_quality_output_acc: 0.5526 - age_output_acc: 0.3933 - weight_output_acc: 0.6378 - bag_output_acc: 0.5651 - footwear_output_acc: 0.4238 - pose_output_acc: 0.6197 - emotion_output_acc: 0.7111 - val_loss: 8.0813 - val_gender_output_loss: 0.6892 - val_image_quality_output_loss: 0.9971 - val_age_output_loss: 1.4318 - val_weight_output_loss: 1.0190 - val_bag_output_loss: 0.9572 - val_footwear_output_loss: 1.0613 - val_pose_output_loss: 0.9409 - val_emotion_output_loss: 0.9848 - val_gender_output_acc: 0.5655 - val_image_quality_output_acc: 0.5570 - val_age_output_acc: 0.4153 - val_weight_output_acc: 0.6250 - val_bag_output_acc: 0.5554 - val_footwear_output_acc: 0.3710 - val_pose_output_acc: 0.6048 - val_emotion_output_acc: 0.7147\n",
            "Learning rate:  0.001\n",
            "Epoch 22/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 232s 644ms/step - loss: 7.9170 - gender_output_loss: 0.6907 - image_quality_output_loss: 0.9876 - age_output_loss: 1.4389 - weight_output_loss: 0.9873 - bag_output_loss: 0.9199 - footwear_output_loss: 1.0490 - pose_output_loss: 0.9305 - emotion_output_loss: 0.9131 - gender_output_acc: 0.5444 - image_quality_output_acc: 0.5529 - age_output_acc: 0.3934 - weight_output_acc: 0.6375 - bag_output_acc: 0.5626 - footwear_output_acc: 0.4205 - pose_output_acc: 0.6198 - emotion_output_acc: 0.7111 - val_loss: 7.8967 - val_gender_output_loss: 0.6868 - val_image_quality_output_loss: 0.9781 - val_age_output_loss: 1.4131 - val_weight_output_loss: 0.9953 - val_bag_output_loss: 0.9389 - val_footwear_output_loss: 1.0352 - val_pose_output_loss: 0.9434 - val_emotion_output_loss: 0.9059 - val_gender_output_acc: 0.5605 - val_image_quality_output_acc: 0.5585 - val_age_output_acc: 0.4153 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.5554 - val_footwear_output_acc: 0.4572 - val_pose_output_acc: 0.6064 - val_emotion_output_acc: 0.7137\n",
            "Learning rate:  0.001\n",
            "Epoch 23/50\n",
            "Learning rate:  0.001\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.9192 - gender_output_loss: 0.6889 - image_quality_output_loss: 0.9905 - age_output_loss: 1.4414 - weight_output_loss: 0.9868 - bag_output_loss: 0.9209 - footwear_output_loss: 1.0490 - pose_output_loss: 0.9289 - emotion_output_loss: 0.9128 - gender_output_acc: 0.5562 - image_quality_output_acc: 0.5528 - age_output_acc: 0.3959 - weight_output_acc: 0.6376 - bag_output_acc: 0.5652 - footwear_output_acc: 0.4276 - pose_output_acc: 0.6196 - emotion_output_acc: 0.7117Epoch 23/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 233s 646ms/step - loss: 7.9203 - gender_output_loss: 0.6888 - image_quality_output_loss: 0.9905 - age_output_loss: 1.4418 - weight_output_loss: 0.9873 - bag_output_loss: 0.9211 - footwear_output_loss: 1.0490 - pose_output_loss: 0.9288 - emotion_output_loss: 0.9130 - gender_output_acc: 0.5564 - image_quality_output_acc: 0.5528 - age_output_acc: 0.3955 - weight_output_acc: 0.6375 - bag_output_acc: 0.5653 - footwear_output_acc: 0.4276 - pose_output_acc: 0.6196 - emotion_output_acc: 0.7116 - val_loss: 7.9805 - val_gender_output_loss: 0.6860 - val_image_quality_output_loss: 0.9820 - val_age_output_loss: 1.4079 - val_weight_output_loss: 1.0059 - val_bag_output_loss: 0.9907 - val_footwear_output_loss: 1.0511 - val_pose_output_loss: 0.9475 - val_emotion_output_loss: 0.9093 - val_gender_output_acc: 0.5605 - val_image_quality_output_acc: 0.5544 - val_age_output_acc: 0.4178 - val_weight_output_acc: 0.6265 - val_bag_output_acc: 0.5534 - val_footwear_output_acc: 0.4567 - val_pose_output_acc: 0.6013 - val_emotion_output_acc: 0.7172\n",
            "Epoch 24/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 232s 645ms/step - loss: 7.9153 - gender_output_loss: 0.6885 - image_quality_output_loss: 0.9882 - age_output_loss: 1.4396 - weight_output_loss: 0.9881 - bag_output_loss: 0.9216 - footwear_output_loss: 1.0490 - pose_output_loss: 0.9274 - emotion_output_loss: 0.9128 - gender_output_acc: 0.5535 - image_quality_output_acc: 0.5528 - age_output_acc: 0.3920 - weight_output_acc: 0.6376 - bag_output_acc: 0.5583 - footwear_output_acc: 0.4343 - pose_output_acc: 0.6197 - emotion_output_acc: 0.7112 - val_loss: 8.0116 - val_gender_output_loss: 0.6893 - val_image_quality_output_loss: 0.9890 - val_age_output_loss: 1.4305 - val_weight_output_loss: 1.0565 - val_bag_output_loss: 0.9377 - val_footwear_output_loss: 1.0406 - val_pose_output_loss: 0.9434 - val_emotion_output_loss: 0.9247 - val_gender_output_acc: 0.5600 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.4183 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.5514 - val_footwear_output_acc: 0.4587 - val_pose_output_acc: 0.6084 - val_emotion_output_acc: 0.7147\n",
            "Epoch 25/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 232s 645ms/step - loss: 7.9145 - gender_output_loss: 0.6894 - image_quality_output_loss: 0.9874 - age_output_loss: 1.4401 - weight_output_loss: 0.9874 - bag_output_loss: 0.9201 - footwear_output_loss: 1.0499 - pose_output_loss: 0.9282 - emotion_output_loss: 0.9120 - gender_output_acc: 0.5573 - image_quality_output_acc: 0.5525 - age_output_acc: 0.3927 - weight_output_acc: 0.6375 - bag_output_acc: 0.5653 - footwear_output_acc: 0.4229 - pose_output_acc: 0.6200 - emotion_output_acc: 0.7111 - val_loss: 7.9360 - val_gender_output_loss: 0.6881 - val_image_quality_output_loss: 0.9819 - val_age_output_loss: 1.4089 - val_weight_output_loss: 1.0094 - val_bag_output_loss: 0.9339 - val_footwear_output_loss: 1.0315 - val_pose_output_loss: 0.9791 - val_emotion_output_loss: 0.9031 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5570 - val_age_output_acc: 0.4153 - val_weight_output_acc: 0.6250 - val_bag_output_acc: 0.5529 - val_footwear_output_acc: 0.4561 - val_pose_output_acc: 0.6038 - val_emotion_output_acc: 0.7152\n",
            "Epoch 25/50\n",
            "Epoch 26/50\n",
            "Learning rate:  0.001\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.9102 - gender_output_loss: 0.6887 - image_quality_output_loss: 0.9877 - age_output_loss: 1.4379 - weight_output_loss: 0.9861 - bag_output_loss: 0.9199 - footwear_output_loss: 1.0470 - pose_output_loss: 0.9288 - emotion_output_loss: 0.9139 - gender_output_acc: 0.5578 - image_quality_output_acc: 0.5528 - age_output_acc: 0.3954 - weight_output_acc: 0.6374 - bag_output_acc: 0.5628 - footwear_output_acc: 0.4328 - pose_output_acc: 0.6198 - emotion_output_acc: 0.7112Learning rate:  0.001\n",
            "360/360 [==============================] - 232s 645ms/step - loss: 7.9085 - gender_output_loss: 0.6887 - image_quality_output_loss: 0.9876 - age_output_loss: 1.4379 - weight_output_loss: 0.9854 - bag_output_loss: 0.9193 - footwear_output_loss: 1.0466 - pose_output_loss: 0.9287 - emotion_output_loss: 0.9143 - gender_output_acc: 0.5580 - image_quality_output_acc: 0.5528 - age_output_acc: 0.3956 - weight_output_acc: 0.6375 - bag_output_acc: 0.5633 - footwear_output_acc: 0.4332 - pose_output_acc: 0.6199 - emotion_output_acc: 0.7111 - val_loss: 39.6829 - val_gender_output_loss: 4.1857 - val_image_quality_output_loss: 4.7296 - val_age_output_loss: 6.9940 - val_weight_output_loss: 5.6048 - val_bag_output_loss: 5.0912 - val_footwear_output_loss: 3.3990 - val_pose_output_loss: 5.0722 - val_emotion_output_loss: 4.6063 - val_gender_output_acc: 0.5645 - val_image_quality_output_acc: 0.5544 - val_age_output_acc: 0.4168 - val_weight_output_acc: 0.6265 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.4546 - val_pose_output_acc: 0.6094 - val_emotion_output_acc: 0.7142\n",
            "Epoch 27/50\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 232s 644ms/step - loss: 7.8739 - gender_output_loss: 0.6860 - image_quality_output_loss: 0.9832 - age_output_loss: 1.4336 - weight_output_loss: 0.9817 - bag_output_loss: 0.9147 - footwear_output_loss: 1.0424 - pose_output_loss: 0.9245 - emotion_output_loss: 0.9078 - gender_output_acc: 0.5631 - image_quality_output_acc: 0.5529 - age_output_acc: 0.3955 - weight_output_acc: 0.6372 - bag_output_acc: 0.5651 - footwear_output_acc: 0.4432 - pose_output_acc: 0.6196 - emotion_output_acc: 0.7114 - val_loss: 7.8691 - val_gender_output_loss: 0.6866 - val_image_quality_output_loss: 0.9781 - val_age_output_loss: 1.4085 - val_weight_output_loss: 0.9961 - val_bag_output_loss: 0.9351 - val_footwear_output_loss: 1.0313 - val_pose_output_loss: 0.9398 - val_emotion_output_loss: 0.8935 - val_gender_output_acc: 0.5600 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.4189 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5539 - val_footwear_output_acc: 0.4561 - val_pose_output_acc: 0.6064 - val_emotion_output_acc: 0.7177\n",
            "Learning rate:  0.0001\n",
            "Epoch 28/50\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 232s 646ms/step - loss: 7.8696 - gender_output_loss: 0.6855 - image_quality_output_loss: 0.9833 - age_output_loss: 1.4313 - weight_output_loss: 0.9803 - bag_output_loss: 0.9143 - footwear_output_loss: 1.0425 - pose_output_loss: 0.9242 - emotion_output_loss: 0.9081 - gender_output_acc: 0.5632 - image_quality_output_acc: 0.5527 - age_output_acc: 0.3954 - weight_output_acc: 0.6378 - bag_output_acc: 0.5654 - footwear_output_acc: 0.4429 - pose_output_acc: 0.6196 - emotion_output_acc: 0.7109 - val_loss: 7.8800 - val_gender_output_loss: 0.6878 - val_image_quality_output_loss: 0.9774 - val_age_output_loss: 1.4077 - val_weight_output_loss: 0.9917 - val_bag_output_loss: 0.9343 - val_footwear_output_loss: 1.0322 - val_pose_output_loss: 0.9399 - val_emotion_output_loss: 0.9089 - val_gender_output_acc: 0.5600 - val_image_quality_output_acc: 0.5575 - val_age_output_acc: 0.4178 - val_weight_output_acc: 0.6270 - val_bag_output_acc: 0.5519 - val_footwear_output_acc: 0.4556 - val_pose_output_acc: 0.6064 - val_emotion_output_acc: 0.7102\n",
            "Epoch 29/50\n",
            "Learning rate:  0.0001\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.8697 - gender_output_loss: 0.6858 - image_quality_output_loss: 0.9827 - age_output_loss: 1.4314 - weight_output_loss: 0.9804 - bag_output_loss: 0.9146 - footwear_output_loss: 1.0426 - pose_output_loss: 0.9239 - emotion_output_loss: 0.9084 - gender_output_acc: 0.5631 - image_quality_output_acc: 0.5531 - age_output_acc: 0.3958 - weight_output_acc: 0.6376 - bag_output_acc: 0.5652 - footwear_output_acc: 0.4429 - pose_output_acc: 0.6200 - emotion_output_acc: 0.7107Epoch 29/50\n",
            "360/360 [==============================] - 233s 646ms/step - loss: 7.8703 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9830 - age_output_loss: 1.4318 - weight_output_loss: 0.9809 - bag_output_loss: 0.9144 - footwear_output_loss: 1.0427 - pose_output_loss: 0.9240 - emotion_output_loss: 0.9079 - gender_output_acc: 0.5632 - image_quality_output_acc: 0.5529 - age_output_acc: 0.3956 - weight_output_acc: 0.6372 - bag_output_acc: 0.5652 - footwear_output_acc: 0.4429 - pose_output_acc: 0.6199 - emotion_output_acc: 0.7110 - val_loss: 7.8631 - val_gender_output_loss: 0.6863 - val_image_quality_output_loss: 0.9768 - val_age_output_loss: 1.4060 - val_weight_output_loss: 0.9889 - val_bag_output_loss: 0.9330 - val_footwear_output_loss: 1.0297 - val_pose_output_loss: 0.9399 - val_emotion_output_loss: 0.9025 - val_gender_output_acc: 0.5600 - val_image_quality_output_acc: 0.5575 - val_age_output_acc: 0.4168 - val_weight_output_acc: 0.6280 - val_bag_output_acc: 0.5539 - val_footwear_output_acc: 0.4587 - val_pose_output_acc: 0.6058 - val_emotion_output_acc: 0.7127\n",
            "Epoch 30/50\n",
            "Learning rate:  0.0001\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.8711 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9833 - age_output_loss: 1.4323 - weight_output_loss: 0.9810 - bag_output_loss: 0.9142 - footwear_output_loss: 1.0426 - pose_output_loss: 0.9237 - emotion_output_loss: 0.9086 - gender_output_acc: 0.5631 - image_quality_output_acc: 0.5531 - age_output_acc: 0.3956 - weight_output_acc: 0.6375 - bag_output_acc: 0.5653 - footwear_output_acc: 0.4430 - pose_output_acc: 0.6200 - emotion_output_acc: 0.7109Epoch 30/50\n",
            "360/360 [==============================] - 233s 648ms/step - loss: 7.8710 - gender_output_loss: 0.6855 - image_quality_output_loss: 0.9835 - age_output_loss: 1.4321 - weight_output_loss: 0.9807 - bag_output_loss: 0.9146 - footwear_output_loss: 1.0425 - pose_output_loss: 0.9240 - emotion_output_loss: 0.9081 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5529 - age_output_acc: 0.3957 - weight_output_acc: 0.6377 - bag_output_acc: 0.5648 - footwear_output_acc: 0.4431 - pose_output_acc: 0.6199 - emotion_output_acc: 0.7111 - val_loss: 7.8747 - val_gender_output_loss: 0.6873 - val_image_quality_output_loss: 0.9758 - val_age_output_loss: 1.4035 - val_weight_output_loss: 0.9961 - val_bag_output_loss: 0.9352 - val_footwear_output_loss: 1.0339 - val_pose_output_loss: 0.9405 - val_emotion_output_loss: 0.9025 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5570 - val_age_output_acc: 0.4199 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.5539 - val_footwear_output_acc: 0.4572 - val_pose_output_acc: 0.6053 - val_emotion_output_acc: 0.7132\n",
            "Epoch 31/50\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 233s 648ms/step - loss: 7.8710 - gender_output_loss: 0.6855 - image_quality_output_loss: 0.9835 - age_output_loss: 1.4321 - weight_output_loss: 0.9807 - bag_output_loss: 0.9146 - footwear_output_loss: 1.0425 - pose_output_loss: 0.9240 - emotion_output_loss: 0.9081 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5529 - age_output_acc: 0.3957 - weight_output_acc: 0.6377 - bag_output_acc: 0.5648 - footwear_output_acc: 0.4431 - pose_output_acc: 0.6199 - emotion_output_acc: 0.7111 - val_loss: 7.8747 - val_gender_output_loss: 0.6873 - val_image_quality_output_loss: 0.9758 - val_age_output_loss: 1.4035 - val_weight_output_loss: 0.9961 - val_bag_output_loss: 0.9352 - val_footwear_output_loss: 1.0339 - val_pose_output_loss: 0.9405 - val_emotion_output_loss: 0.9025 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5570 - val_age_output_acc: 0.4199 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.5539 - val_footwear_output_acc: 0.4572 - val_pose_output_acc: 0.6053 - val_emotion_output_acc: 0.7132\n",
            "360/360 [==============================] - 234s 650ms/step - loss: 7.8701 - gender_output_loss: 0.6861 - image_quality_output_loss: 0.9833 - age_output_loss: 1.4319 - weight_output_loss: 0.9807 - bag_output_loss: 0.9144 - footwear_output_loss: 1.0423 - pose_output_loss: 0.9242 - emotion_output_loss: 0.9072 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5529 - age_output_acc: 0.3956 - weight_output_acc: 0.6377 - bag_output_acc: 0.5652 - footwear_output_acc: 0.4431 - pose_output_acc: 0.6196 - emotion_output_acc: 0.7114 - val_loss: 7.8924 - val_gender_output_loss: 0.6868 - val_image_quality_output_loss: 0.9785 - val_age_output_loss: 1.4080 - val_weight_output_loss: 1.0047 - val_bag_output_loss: 0.9351 - val_footwear_output_loss: 1.0336 - val_pose_output_loss: 0.9427 - val_emotion_output_loss: 0.9032 - val_gender_output_acc: 0.5575 - val_image_quality_output_acc: 0.5549 - val_age_output_acc: 0.4163 - val_weight_output_acc: 0.6215 - val_bag_output_acc: 0.5514 - val_footwear_output_acc: 0.4567 - val_pose_output_acc: 0.6033 - val_emotion_output_acc: 0.7127\n",
            "Epoch 32/50\n",
            "Learning rate:  0.0001\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.8707 - gender_output_loss: 0.6856 - image_quality_output_loss: 0.9828 - age_output_loss: 1.4318 - weight_output_loss: 0.9816 - bag_output_loss: 0.9142 - footwear_output_loss: 1.0426 - pose_output_loss: 0.9242 - emotion_output_loss: 0.9078 - gender_output_acc: 0.5632 - image_quality_output_acc: 0.5529 - age_output_acc: 0.3955 - weight_output_acc: 0.6373 - bag_output_acc: 0.5654 - footwear_output_acc: 0.4429 - pose_output_acc: 0.6199 - emotion_output_acc: 0.7112Epoch 32/50\n",
            "360/360 [==============================] - 235s 652ms/step - loss: 7.8703 - gender_output_loss: 0.6856 - image_quality_output_loss: 0.9827 - age_output_loss: 1.4318 - weight_output_loss: 0.9814 - bag_output_loss: 0.9146 - footwear_output_loss: 1.0425 - pose_output_loss: 0.9242 - emotion_output_loss: 0.9076 - gender_output_acc: 0.5631 - image_quality_output_acc: 0.5530 - age_output_acc: 0.3957 - weight_output_acc: 0.6374 - bag_output_acc: 0.5651 - footwear_output_acc: 0.4431 - pose_output_acc: 0.6199 - emotion_output_acc: 0.7113 - val_loss: 7.8670 - val_gender_output_loss: 0.6871 - val_image_quality_output_loss: 0.9791 - val_age_output_loss: 1.4097 - val_weight_output_loss: 0.9922 - val_bag_output_loss: 0.9320 - val_footwear_output_loss: 1.0332 - val_pose_output_loss: 0.9395 - val_emotion_output_loss: 0.8942 - val_gender_output_acc: 0.5605 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.4153 - val_weight_output_acc: 0.6270 - val_bag_output_acc: 0.5559 - val_footwear_output_acc: 0.4546 - val_pose_output_acc: 0.6053 - val_emotion_output_acc: 0.7167\n",
            "Epoch 33/50\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 233s 648ms/step - loss: 7.8706 - gender_output_loss: 0.6860 - image_quality_output_loss: 0.9832 - age_output_loss: 1.4317 - weight_output_loss: 0.9814 - bag_output_loss: 0.9150 - footwear_output_loss: 1.0426 - pose_output_loss: 0.9239 - emotion_output_loss: 0.9069 - gender_output_acc: 0.5631 - image_quality_output_acc: 0.5528 - age_output_acc: 0.3957 - weight_output_acc: 0.6373 - bag_output_acc: 0.5651 - footwear_output_acc: 0.4431 - pose_output_acc: 0.6200 - emotion_output_acc: 0.7115 - val_loss: 7.9649 - val_gender_output_loss: 0.6870 - val_image_quality_output_loss: 0.9855 - val_age_output_loss: 1.4223 - val_weight_output_loss: 1.0143 - val_bag_output_loss: 0.9378 - val_footwear_output_loss: 1.0372 - val_pose_output_loss: 0.9484 - val_emotion_output_loss: 0.9324 - val_gender_output_acc: 0.5585 - val_image_quality_output_acc: 0.5575 - val_age_output_acc: 0.4183 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.4556 - val_pose_output_acc: 0.6048 - val_emotion_output_acc: 0.7147\n",
            "Epoch 34/50\n",
            "Learning rate:  0.0001\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.8712 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9830 - age_output_loss: 1.4324 - weight_output_loss: 0.9818 - bag_output_loss: 0.9145 - footwear_output_loss: 1.0422 - pose_output_loss: 0.9243 - emotion_output_loss: 0.9073 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5528 - age_output_acc: 0.3952 - weight_output_acc: 0.6368 - bag_output_acc: 0.5654 - footwear_output_acc: 0.4425 - pose_output_acc: 0.6199 - emotion_output_acc: 0.7114Epoch 34/50\n",
            " 0.0001\n",
            "360/360 [==============================] - 234s 649ms/step - loss: 7.8708 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9827 - age_output_loss: 1.4322 - weight_output_loss: 0.9813 - bag_output_loss: 0.9145 - footwear_output_loss: 1.0422 - pose_output_loss: 0.9243 - emotion_output_loss: 0.9079 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5530 - age_output_acc: 0.3955 - weight_output_acc: 0.6371 - bag_output_acc: 0.5652 - footwear_output_acc: 0.4428 - pose_output_acc: 0.6199 - emotion_output_acc: 0.7111 - val_loss: 7.9272 - val_gender_output_loss: 0.6864 - val_image_quality_output_loss: 0.9881 - val_age_output_loss: 1.4157 - val_weight_output_loss: 1.0000 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 1.0344 - val_pose_output_loss: 0.9473 - val_emotion_output_loss: 0.9193 - val_gender_output_acc: 0.5585 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.4209 - val_weight_output_acc: 0.6300 - val_bag_output_acc: 0.5554 - val_footwear_output_acc: 0.4551 - val_pose_output_acc: 0.6038 - val_emotion_output_acc: 0.7147\n",
            "Epoch 35/50\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 233s 648ms/step - loss: 7.8695 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9830 - age_output_loss: 1.4317 - weight_output_loss: 0.9805 - bag_output_loss: 0.9147 - footwear_output_loss: 1.0423 - pose_output_loss: 0.9242 - emotion_output_loss: 0.9073 - gender_output_acc: 0.5630 - image_quality_output_acc: 0.5529 - age_output_acc: 0.3956 - weight_output_acc: 0.6374 - bag_output_acc: 0.5648 - footwear_output_acc: 0.4432 - pose_output_acc: 0.6198 - emotion_output_acc: 0.7112 - val_loss: 7.8705 - val_gender_output_loss: 0.6855 - val_image_quality_output_loss: 0.9809 - val_age_output_loss: 1.4078 - val_weight_output_loss: 0.9929 - val_bag_output_loss: 0.9351 - val_footwear_output_loss: 1.0326 - val_pose_output_loss: 0.9382 - val_emotion_output_loss: 0.8975 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5529 - val_age_output_acc: 0.4168 - val_weight_output_acc: 0.6265 - val_bag_output_acc: 0.5514 - val_footwear_output_acc: 0.4556 - val_pose_output_acc: 0.6069 - val_emotion_output_acc: 0.7157\n",
            "Epoch 36/50\n",
            "Learning rate:  0.0001\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.8685 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9830 - age_output_loss: 1.4315 - weight_output_loss: 0.9809 - bag_output_loss: 0.9139 - footwear_output_loss: 1.0425 - pose_output_loss: 0.9237 - emotion_output_loss: 0.9076 - gender_output_acc: 0.5626 - image_quality_output_acc: 0.5527 - age_output_acc: 0.3955 - weight_output_acc: 0.6374 - bag_output_acc: 0.5652 - footwear_output_acc: 0.4422 - pose_output_acc: 0.6202 - emotion_output_acc: 0.7113Epoch 36/50\n",
            "360/360 [==============================] - 233s 649ms/step - loss: 7.8700 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9832 - age_output_loss: 1.4317 - weight_output_loss: 0.9811 - bag_output_loss: 0.9143 - footwear_output_loss: 1.0423 - pose_output_loss: 0.9241 - emotion_output_loss: 0.9080 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5524 - age_output_acc: 0.3955 - weight_output_acc: 0.6373 - bag_output_acc: 0.5652 - footwear_output_acc: 0.4426 - pose_output_acc: 0.6199 - emotion_output_acc: 0.7110 - val_loss: 7.8695 - val_gender_output_loss: 0.6866 - val_image_quality_output_loss: 0.9758 - val_age_output_loss: 1.4091 - val_weight_output_loss: 0.9916 - val_bag_output_loss: 0.9337 - val_footwear_output_loss: 1.0311 - val_pose_output_loss: 0.9436 - val_emotion_output_loss: 0.8981 - val_gender_output_acc: 0.5585 - val_image_quality_output_acc: 0.5570 - val_age_output_acc: 0.4168 - val_weight_output_acc: 0.6300 - val_bag_output_acc: 0.5544 - val_footwear_output_acc: 0.4546 - val_pose_output_acc: 0.6048 - val_emotion_output_acc: 0.7147\n",
            "Epoch 37/50\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 234s 649ms/step - loss: 7.8716 - gender_output_loss: 0.6859 - image_quality_output_loss: 0.9836 - age_output_loss: 1.4320 - weight_output_loss: 0.9808 - bag_output_loss: 0.9145 - footwear_output_loss: 1.0423 - pose_output_loss: 0.9245 - emotion_output_loss: 0.9080 - gender_output_acc: 0.5630 - image_quality_output_acc: 0.5527 - age_output_acc: 0.3956 - weight_output_acc: 0.6375 - bag_output_acc: 0.5652 - footwear_output_acc: 0.4431 - pose_output_acc: 0.6196 - emotion_output_acc: 0.7112 - val_loss: 7.8818 - val_gender_output_loss: 0.6856 - val_image_quality_output_loss: 0.9757 - val_age_output_loss: 1.4111 - val_weight_output_loss: 0.9946 - val_bag_output_loss: 0.9351 - val_footwear_output_loss: 1.0336 - val_pose_output_loss: 0.9393 - val_emotion_output_loss: 0.9068 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5575 - val_age_output_acc: 0.4163 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5514 - val_footwear_output_acc: 0.4526 - val_pose_output_acc: 0.6058 - val_emotion_output_acc: 0.7107\n",
            "Epoch 38/50\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 233s 648ms/step - loss: 7.8712 - gender_output_loss: 0.6858 - image_quality_output_loss: 0.9833 - age_output_loss: 1.4319 - weight_output_loss: 0.9808 - bag_output_loss: 0.9147 - footwear_output_loss: 1.0428 - pose_output_loss: 0.9246 - emotion_output_loss: 0.9074 - gender_output_acc: 0.5629 - image_quality_output_acc: 0.5527 - age_output_acc: 0.3957 - weight_output_acc: 0.6376 - bag_output_acc: 0.5650 - footwear_output_acc: 0.4428 - pose_output_acc: 0.6194 - emotion_output_acc: 0.7114 - val_loss: 7.8971 - val_gender_output_loss: 0.6859 - val_image_quality_output_loss: 0.9776 - val_age_output_loss: 1.4044 - val_weight_output_loss: 0.9957 - val_bag_output_loss: 0.9416 - val_footwear_output_loss: 1.0303 - val_pose_output_loss: 0.9509 - val_emotion_output_loss: 0.9107 - val_gender_output_acc: 0.5605 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.4178 - val_weight_output_acc: 0.6265 - val_bag_output_acc: 0.5529 - val_footwear_output_acc: 0.4602 - val_pose_output_acc: 0.6033 - val_emotion_output_acc: 0.7137\n",
            "Learning rate:  0.0001\n",
            "Epoch 39/50\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 234s 650ms/step - loss: 7.8716 - gender_output_loss: 0.6859 - image_quality_output_loss: 0.9836 - age_output_loss: 1.4321 - weight_output_loss: 0.9812 - bag_output_loss: 0.9146 - footwear_output_loss: 1.0422 - pose_output_loss: 0.9242 - emotion_output_loss: 0.9077 - gender_output_acc: 0.5631 - image_quality_output_acc: 0.5526 - age_output_acc: 0.3955 - weight_output_acc: 0.6372 - bag_output_acc: 0.5650 - footwear_output_acc: 0.4428 - pose_output_acc: 0.6199 - emotion_output_acc: 0.7112 - val_loss: 7.8699 - val_gender_output_loss: 0.6858 - val_image_quality_output_loss: 0.9775 - val_age_output_loss: 1.4085 - val_weight_output_loss: 0.9871 - val_bag_output_loss: 0.9372 - val_footwear_output_loss: 1.0348 - val_pose_output_loss: 0.9437 - val_emotion_output_loss: 0.8953 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.4163 - val_weight_output_acc: 0.6295 - val_bag_output_acc: 0.5559 - val_footwear_output_acc: 0.4572 - val_pose_output_acc: 0.6053 - val_emotion_output_acc: 0.7157\n",
            "Epoch 40/50\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 234s 649ms/step - loss: 7.8705 - gender_output_loss: 0.6860 - image_quality_output_loss: 0.9833 - age_output_loss: 1.4318 - weight_output_loss: 0.9810 - bag_output_loss: 0.9148 - footwear_output_loss: 1.0427 - pose_output_loss: 0.9241 - emotion_output_loss: 0.9069 - gender_output_acc: 0.5633 - image_quality_output_acc: 0.5530 - age_output_acc: 0.3955 - weight_output_acc: 0.6374 - bag_output_acc: 0.5652 - footwear_output_acc: 0.4391 - pose_output_acc: 0.6196 - emotion_output_acc: 0.7113 - val_loss: 7.8689 - val_gender_output_loss: 0.6882 - val_image_quality_output_loss: 0.9774 - val_age_output_loss: 1.4051 - val_weight_output_loss: 0.9939 - val_bag_output_loss: 0.9334 - val_footwear_output_loss: 1.0287 - val_pose_output_loss: 0.9454 - val_emotion_output_loss: 0.8968 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5570 - val_age_output_acc: 0.4168 - val_weight_output_acc: 0.6280 - val_bag_output_acc: 0.5554 - val_footwear_output_acc: 0.4572 - val_pose_output_acc: 0.6038 - val_emotion_output_acc: 0.7167\n",
            "Epoch 41/50\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 234s 649ms/step - loss: 7.8704 - gender_output_loss: 0.6858 - image_quality_output_loss: 0.9832 - age_output_loss: 1.4319 - weight_output_loss: 0.9805 - bag_output_loss: 0.9141 - footwear_output_loss: 1.0427 - pose_output_loss: 0.9243 - emotion_output_loss: 0.9080 - gender_output_acc: 0.5632 - image_quality_output_acc: 0.5527 - age_output_acc: 0.3955 - weight_output_acc: 0.6377 - bag_output_acc: 0.5653 - footwear_output_acc: 0.4428 - pose_output_acc: 0.6199 - emotion_output_acc: 0.7111 - val_loss: 7.8707 - val_gender_output_loss: 0.6862 - val_image_quality_output_loss: 0.9768 - val_age_output_loss: 1.4082 - val_weight_output_loss: 0.9942 - val_bag_output_loss: 0.9325 - val_footwear_output_loss: 1.0332 - val_pose_output_loss: 0.9397 - val_emotion_output_loss: 0.9000 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5585 - val_age_output_acc: 0.4163 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.5509 - val_footwear_output_acc: 0.4546 - val_pose_output_acc: 0.6053 - val_emotion_output_acc: 0.7147\n",
            "Epoch 42/50\n",
            "Learning rate:  1e-05\n",
            "360/360 [==============================] - 233s 646ms/step - loss: 7.8672 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9828 - age_output_loss: 1.4311 - weight_output_loss: 0.9806 - bag_output_loss: 0.9140 - footwear_output_loss: 1.0422 - pose_output_loss: 0.9237 - emotion_output_loss: 0.9074 - gender_output_acc: 0.5630 - image_quality_output_acc: 0.5526 - age_output_acc: 0.3955 - weight_output_acc: 0.6373 - bag_output_acc: 0.5652 - footwear_output_acc: 0.4430 - pose_output_acc: 0.6198 - emotion_output_acc: 0.7110 - val_loss: 7.8616 - val_gender_output_loss: 0.6857 - val_image_quality_output_loss: 0.9781 - val_age_output_loss: 1.4080 - val_weight_output_loss: 0.9918 - val_bag_output_loss: 0.9324 - val_footwear_output_loss: 1.0312 - val_pose_output_loss: 0.9359 - val_emotion_output_loss: 0.8985 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.4168 - val_weight_output_acc: 0.6260 - val_bag_output_acc: 0.5534 - val_footwear_output_acc: 0.4577 - val_pose_output_acc: 0.6089 - val_emotion_output_acc: 0.7147\n",
            "Epoch 43/50\n",
            "Learning rate:  1e-05\n",
            "360/360 [==============================] - 234s 650ms/step - loss: 7.8657 - gender_output_loss: 0.6852 - image_quality_output_loss: 0.9826 - age_output_loss: 1.4310 - weight_output_loss: 0.9797 - bag_output_loss: 0.9142 - footwear_output_loss: 1.0421 - pose_output_loss: 0.9235 - emotion_output_loss: 0.9073 - gender_output_acc: 0.5630 - image_quality_output_acc: 0.5527 - age_output_acc: 0.3957 - weight_output_acc: 0.6378 - bag_output_acc: 0.5649 - footwear_output_acc: 0.4424 - pose_output_acc: 0.6198 - emotion_output_acc: 0.7111 - val_loss: 7.8720 - val_gender_output_loss: 0.6857 - val_image_quality_output_loss: 0.9786 - val_age_output_loss: 1.4066 - val_weight_output_loss: 0.9946 - val_bag_output_loss: 0.9306 - val_footwear_output_loss: 1.0326 - val_pose_output_loss: 0.9398 - val_emotion_output_loss: 0.9035 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.4163 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5559 - val_footwear_output_acc: 0.4526 - val_pose_output_acc: 0.6053 - val_emotion_output_acc: 0.7127\n",
            "Learning rate:  1e-05\n",
            "360/360 [==============================] - 233s 646ms/step - loss: 7.8672 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9828 - age_output_loss: 1.4311 - weight_output_loss: 0.9806 - bag_output_loss: 0.9140 - footwear_output_loss: 1.0422 - pose_output_loss: 0.9237 - emotion_output_loss: 0.9074 - gender_output_acc: 0.5630 - image_quality_output_acc: 0.5526 - age_output_acc: 0.3955 - weight_output_acc: 0.6373 - bag_output_acc: 0.5652 - footwear_output_acc: 0.4430 - pose_output_acc: 0.6198 - emotion_output_acc: 0.7110 - val_loss: 7.8616 - val_gender_output_loss: 0.6857 - val_image_quality_output_loss: 0.9781 - val_age_output_loss: 1.4080 - val_weight_output_loss: 0.9918 - val_bag_output_loss: 0.9324 - val_footwear_output_loss: 1.0312 - val_pose_output_loss: 0.9359 - val_emotion_output_loss: 0.8985 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.4168 - val_weight_output_acc: 0.6260 - val_bag_output_acc: 0.5534 - val_footwear_output_acc: 0.4577 - val_pose_output_acc: 0.6089 - val_emotion_output_acc: 0.7147\n",
            "Epoch 44/50\n",
            "Learning rate:  1e-05\n",
            "360/360 [==============================] - 233s 646ms/step - loss: 7.8665 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9825 - age_output_loss: 1.4310 - weight_output_loss: 0.9804 - bag_output_loss: 0.9141 - footwear_output_loss: 1.0419 - pose_output_loss: 0.9239 - emotion_output_loss: 0.9075 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5529 - age_output_acc: 0.3954 - weight_output_acc: 0.6374 - bag_output_acc: 0.5651 - footwear_output_acc: 0.4428 - pose_output_acc: 0.6195 - emotion_output_acc: 0.7109 - val_loss: 7.8618 - val_gender_output_loss: 0.6865 - val_image_quality_output_loss: 0.9748 - val_age_output_loss: 1.4058 - val_weight_output_loss: 0.9903 - val_bag_output_loss: 0.9335 - val_footwear_output_loss: 1.0322 - val_pose_output_loss: 0.9345 - val_emotion_output_loss: 0.9041 - val_gender_output_acc: 0.5575 - val_image_quality_output_acc: 0.5580 - val_age_output_acc: 0.4158 - val_weight_output_acc: 0.6260 - val_bag_output_acc: 0.5519 - val_footwear_output_acc: 0.4546 - val_pose_output_acc: 0.6099 - val_emotion_output_acc: 0.7122\n",
            "Epoch 45/50\n",
            "Learning rate:  1e-05\n",
            "360/360 [==============================] - 233s 648ms/step - loss: 7.8648 - gender_output_loss: 0.6852 - image_quality_output_loss: 0.9829 - age_output_loss: 1.4311 - weight_output_loss: 0.9794 - bag_output_loss: 0.9141 - footwear_output_loss: 1.0418 - pose_output_loss: 0.9233 - emotion_output_loss: 0.9070 - gender_output_acc: 0.5633 - image_quality_output_acc: 0.5524 - age_output_acc: 0.3954 - weight_output_acc: 0.6377 - bag_output_acc: 0.5651 - footwear_output_acc: 0.4428 - pose_output_acc: 0.6201 - emotion_output_acc: 0.7111 - val_loss: 7.8725 - val_gender_output_loss: 0.6860 - val_image_quality_output_loss: 0.9787 - val_age_output_loss: 1.4088 - val_weight_output_loss: 0.9919 - val_bag_output_loss: 0.9342 - val_footwear_output_loss: 1.0335 - val_pose_output_loss: 0.9399 - val_emotion_output_loss: 0.8996 - val_gender_output_acc: 0.5600 - val_image_quality_output_acc: 0.5549 - val_age_output_acc: 0.4153 - val_weight_output_acc: 0.6250 - val_bag_output_acc: 0.5524 - val_footwear_output_acc: 0.4536 - val_pose_output_acc: 0.6053 - val_emotion_output_acc: 0.7142\n",
            "Epoch 46/50\n",
            "Learning rate:  1e-05\n",
            "360/360 [==============================] - 234s 649ms/step - loss: 7.8645 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9826 - age_output_loss: 1.4308 - weight_output_loss: 0.9801 - bag_output_loss: 0.9138 - footwear_output_loss: 1.0418 - pose_output_loss: 0.9237 - emotion_output_loss: 0.9064 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5529 - age_output_acc: 0.3956 - weight_output_acc: 0.6376 - bag_output_acc: 0.5654 - footwear_output_acc: 0.4428 - pose_output_acc: 0.6197 - emotion_output_acc: 0.7115 - val_loss: 7.8609 - val_gender_output_loss: 0.6860 - val_image_quality_output_loss: 0.9790 - val_age_output_loss: 1.4057 - val_weight_output_loss: 0.9901 - val_bag_output_loss: 0.9300 - val_footwear_output_loss: 1.0313 - val_pose_output_loss: 0.9418 - val_emotion_output_loss: 0.8969 - val_gender_output_acc: 0.5595 - val_image_quality_output_acc: 0.5544 - val_age_output_acc: 0.4163 - val_weight_output_acc: 0.6265 - val_bag_output_acc: 0.5554 - val_footwear_output_acc: 0.4556 - val_pose_output_acc: 0.6038 - val_emotion_output_acc: 0.7157\n",
            "Learning rate:  1e-05\n",
            "Epoch 47/50\n",
            "Learning rate:  1e-05\n",
            "360/360 [==============================] - 231s 642ms/step - loss: 7.8649 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9823 - age_output_loss: 1.4306 - weight_output_loss: 0.9800 - bag_output_loss: 0.9144 - footwear_output_loss: 1.0419 - pose_output_loss: 0.9235 - emotion_output_loss: 0.9070 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5532 - age_output_acc: 0.3957 - weight_output_acc: 0.6376 - bag_output_acc: 0.5647 - footwear_output_acc: 0.4431 - pose_output_acc: 0.6199 - emotion_output_acc: 0.7112 - val_loss: 7.8634 - val_gender_output_loss: 0.6864 - val_image_quality_output_loss: 0.9794 - val_age_output_loss: 1.4070 - val_weight_output_loss: 0.9919 - val_bag_output_loss: 0.9339 - val_footwear_output_loss: 1.0293 - val_pose_output_loss: 0.9415 - val_emotion_output_loss: 0.8940 - val_gender_output_acc: 0.5585 - val_image_quality_output_acc: 0.5534 - val_age_output_acc: 0.4178 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.5534 - val_footwear_output_acc: 0.4561 - val_pose_output_acc: 0.6043 - val_emotion_output_acc: 0.7172\n",
            "Epoch 48/50\n",
            "Learning rate:  1e-05\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.8638 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9827 - age_output_loss: 1.4313 - weight_output_loss: 0.9797 - bag_output_loss: 0.9141 - footwear_output_loss: 1.0418 - pose_output_loss: 0.9237 - emotion_output_loss: 0.9053 - gender_output_acc: 0.5629 - image_quality_output_acc: 0.5528 - age_output_acc: 0.3953 - weight_output_acc: 0.6376 - bag_output_acc: 0.5652 - footwear_output_acc: 0.4426 - pose_output_acc: 0.6197 - emotion_output_acc: 0.7121Epoch 48/50\n",
            "Learning rate:  1e-05\n",
            "360/360 [==============================] - 234s 651ms/step - loss: 7.8650 - gender_output_loss: 0.6852 - image_quality_output_loss: 0.9825 - age_output_loss: 1.4310 - weight_output_loss: 0.9800 - bag_output_loss: 0.9139 - footwear_output_loss: 1.0419 - pose_output_loss: 0.9239 - emotion_output_loss: 0.9066 - gender_output_acc: 0.5631 - image_quality_output_acc: 0.5530 - age_output_acc: 0.3954 - weight_output_acc: 0.6377 - bag_output_acc: 0.5653 - footwear_output_acc: 0.4427 - pose_output_acc: 0.6195 - emotion_output_acc: 0.7115 - val_loss: 7.8701 - val_gender_output_loss: 0.6862 - val_image_quality_output_loss: 0.9798 - val_age_output_loss: 1.4073 - val_weight_output_loss: 0.9894 - val_bag_output_loss: 0.9328 - val_footwear_output_loss: 1.0315 - val_pose_output_loss: 0.9438 - val_emotion_output_loss: 0.8992 - val_gender_output_acc: 0.5590 - val_image_quality_output_acc: 0.5534 - val_age_output_acc: 0.4163 - val_weight_output_acc: 0.6280 - val_bag_output_acc: 0.5539 - val_footwear_output_acc: 0.4536 - val_pose_output_acc: 0.6018 - val_emotion_output_acc: 0.7147\n",
            "Epoch 49/50\n",
            "Learning rate:  1e-05\n",
            "360/360 [==============================] - 233s 647ms/step - loss: 7.8656 - gender_output_loss: 0.6852 - image_quality_output_loss: 0.9827 - age_output_loss: 1.4309 - weight_output_loss: 0.9801 - bag_output_loss: 0.9138 - footwear_output_loss: 1.0421 - pose_output_loss: 0.9238 - emotion_output_loss: 0.9071 - gender_output_acc: 0.5630 - image_quality_output_acc: 0.5528 - age_output_acc: 0.3956 - weight_output_acc: 0.6375 - bag_output_acc: 0.5653 - footwear_output_acc: 0.4429 - pose_output_acc: 0.6196 - emotion_output_acc: 0.7111 - val_loss: 7.8587 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9771 - val_age_output_loss: 1.4075 - val_weight_output_loss: 0.9941 - val_bag_output_loss: 0.9333 - val_footwear_output_loss: 1.0293 - val_pose_output_loss: 0.9390 - val_emotion_output_loss: 0.8932 - val_gender_output_acc: 0.5630 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.4199 - val_weight_output_acc: 0.6260 - val_bag_output_acc: 0.5539 - val_footwear_output_acc: 0.4567 - val_pose_output_acc: 0.6064 - val_emotion_output_acc: 0.7172\n",
            "Epoch 50/50\n",
            "Learning rate:  1e-05\n",
            "360/360 [==============================] - 229s 637ms/step - loss: 7.8657 - gender_output_loss: 0.6851 - image_quality_output_loss: 0.9825 - age_output_loss: 1.4309 - weight_output_loss: 0.9805 - bag_output_loss: 0.9138 - footwear_output_loss: 1.0417 - pose_output_loss: 0.9239 - emotion_output_loss: 0.9073 - gender_output_acc: 0.5635 - image_quality_output_acc: 0.5527 - age_output_acc: 0.3957 - weight_output_acc: 0.6373 - bag_output_acc: 0.5652 - footwear_output_acc: 0.4432 - pose_output_acc: 0.6195 - emotion_output_acc: 0.7110 - val_loss: 7.8764 - val_gender_output_loss: 0.6857 - val_image_quality_output_loss: 0.9772 - val_age_output_loss: 1.4118 - val_weight_output_loss: 0.9958 - val_bag_output_loss: 0.9317 - val_footwear_output_loss: 1.0313 - val_pose_output_loss: 0.9389 - val_emotion_output_loss: 0.9039 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.4123 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.5554 - val_footwear_output_acc: 0.4567 - val_pose_output_acc: 0.6058 - val_emotion_output_acc: 0.7132\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}